{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ограничения на ресурсы для numpy, импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\" # export NUMEXPR_NUM_THREADS=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from functools import reduce\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from scipy.optimize import minimize\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import QuantileTransformer, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по регионам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 79)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/region_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "reg_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    reg_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(reg_id, return_counts=True)\n",
    "_id_map = {}\n",
    "regionmap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(regionmap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "reg_id = np.array([_id_map.get(cid, len(_id_map)) for cid in reg_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "reg_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по городам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 661)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/city_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "city_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    city_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(city_id, return_counts=True)\n",
    "_id_map = {}\n",
    "citmap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(citmap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "city_id = np.array([_id_map.get(cid, len(_id_map)) for cid in city_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "city_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по производителю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/cpe_manufacturer_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "cpeman_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    cpeman_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(cpeman_id, return_counts=True)\n",
    "_id_map = {}\n",
    "cpemanmap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(cpemanmap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "cpeman_id = np.array([_id_map.get(cid, len(_id_map)) for cid in cpeman_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "cpeman_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по устройству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 396)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/cpe_model_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "cpemodname_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    cpemodname_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(cpemodname_id, return_counts=True)\n",
    "_id_map = {}\n",
    "cpemodnamemap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(cpemodnamemap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "cpemodname_id = np.array([_id_map.get(cid, len(_id_map)) for cid in cpemodname_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "cpemodname_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по cpe_type_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 2, 1, 3], dtype='int64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/cpe_type_cd_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "cpetype_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "cpetypemap = csr_matrix(regmap['data'])\n",
    "print(cpetypemap.shape)\n",
    "pd.Series(\n",
    "    cpetype_id\n",
    ").value_counts().head(50).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по датам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 397)\n",
      "(415317, 203)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/date_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "date_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "datemap = csr_matrix(regmap['data'])\n",
    "print(datemap.shape)\n",
    "pd.Series(\n",
    "    date_id\n",
    ").value_counts().tail(50)\n",
    "\n",
    "_id_label, _id_cnt = np.unique(date_id, return_counts=True)\n",
    "_id_map = {}\n",
    "datemap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(datemap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "date_id = np.array([_id_map.get(cid, len(_id_map)) for cid in date_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "date_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по времени суток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    217543\n",
       "2    126023\n",
       "0     64969\n",
       "3      6782\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/part_of_day_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "pod_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "podmap = csr_matrix(regmap['data'])\n",
    "print(podmap.shape)\n",
    "pd.Series(\n",
    "    pod_id\n",
    ").value_counts().tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по ценам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('files/price_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "prices = np.array(pd.read_csv('files/price_mapper.tsv.gz', sep='\\t').price.fillna(20_000).tolist()\n",
    "                  + [20_000])\n",
    "price_id = regmap['data']\n",
    "price_id = (price_id.dot(prices)/np.array(price_id.sum(axis=1)).flatten())\n",
    "pricemap_id = regmap['data'].dot(KBinsDiscretizer(n_bins=31,\n",
    "                                                  strategy='kmeans',).fit_transform(prices[:, None]**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по ссылкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': <415317x199684 sparse matrix of type '<class 'numpy.uint32'>'\n",
       " \twith 32277669 stored elements in Compressed Sparse Row format>,\n",
       " 'uids': array([     4,     16,     18, ..., 415276, 415288, 415293])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/url_host_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    datamap = pickle.load(f)\n",
    "datamap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков тайтлов ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<415317x13118 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 82784390 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('auxilary/domain20k_title.pickle.gz', 'rb') as f:\n",
    "    titlemap = pickle.load(f)\n",
    "\n",
    "map_df = pd.read_csv('auxilary/url_host_mapper_v2.tsv.gz', sep='\\t')\n",
    "\n",
    "titlemap = \\\n",
    "datamap['data'][:, pd.DataFrame(\n",
    "    dict(url_host=titlemap['domain'])\n",
    "        ).merge(map_df).url_host_idx.values\n",
    "               ].dot(CountVectorizer(ngram_range=(1,2),\n",
    "                                     min_df=2,).\\\n",
    "                     fit_transform(titlemap['title']))\n",
    "\n",
    "titlemap = csr_matrix((np.log2(1+titlemap.data), titlemap.nonzero()),\n",
    "                      shape=titlemap.shape, dtype=np.float32)\n",
    "titlemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка эмбеддингов скриншотов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(415317, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('auxilary/clipVIT_scores_20k.pickle.gz', 'rb') as f:\n",
    "    clipmap = pickle.load(f)\n",
    "\n",
    "clipmap = \\\n",
    "TfidfTransformer(sublinear_tf=True, norm=None).\\\n",
    "    fit_transform(\n",
    "        datamap['data'][:, pd.DataFrame(\n",
    "            dict(url_host=clipmap['domains'])\n",
    "                ).merge(map_df).url_host_idx.values\n",
    "                       ]\n",
    "    ).\\\n",
    "    dot(\n",
    "        np.array(clipmap['scores'])\n",
    "    )\n",
    "\n",
    "clipmap = Normalizer().fit_transform(clipmap)\n",
    "clipmap = np.float32(clipmap)\n",
    "\n",
    "clipmap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('doc2vec_feats_128x4.pickle.gz', 'rb') as f:\n",
    "    doc2vec_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.96 s, sys: 616 ms, total: 3.58 s\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists('html_feats.pickle.gz'):\n",
    "\n",
    "\n",
    "    with gzip.open('auxilary/domain20k_html.txt.gz', 'rt',\n",
    "                   encoding='utf-8') as f:\n",
    "        html_data = f.read().split('\\n=\\n=\\n')\n",
    "        print(len(html_data))\n",
    "\n",
    "    def tokenize(x):\n",
    "        return x[0], re.findall('(?u)\\\\b\\\\w\\\\w+\\\\b', x[-1].lower())\n",
    "\n",
    "    with Pool(20) as pool:\n",
    "        pool_pbar = tqdm()\n",
    "        html_tokens = [None for _ in range(len(html_data[:-1]))]\n",
    "        for i, r in pool.imap(tokenize, enumerate(html_data[:-1])):\n",
    "            html_tokens[i] = r\n",
    "            pool_pbar.update(1)\n",
    "\n",
    "    html_cbag = \\\n",
    "    CountVectorizer(ngram_range=(1,2),\n",
    "                    min_df=300,\n",
    "                    lowercase=False,\n",
    "                    tokenizer=lambda x: x,\n",
    "                    max_df=0.25).\\\n",
    "                   fit_transform(html_tokens)\n",
    "    print(html_cbag.shape)\n",
    "\n",
    "    html_tfidf = TfidfTransformer(sublinear_tf=True).fit_transform(html_cbag)\n",
    "\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    html_svd = TruncatedSVD(n_components=256,\n",
    "                            random_state=10,\n",
    "                            n_iter=3,).fit_transform(html_tfidf)\n",
    "    print(html_svd.shape)\n",
    "\n",
    "    with gzip.open('auxilary/domain20k_title.pickle.gz', 'rb') as f:\n",
    "        htmlmap = pickle.load(f)\n",
    "    htmlmap = \\\n",
    "    TfidfTransformer(sublinear_tf=True, norm=None).\\\n",
    "        fit_transform(\n",
    "            datamap['data'][:, pd.DataFrame(\n",
    "                dict(url_host=htmlmap['domain'])\n",
    "                    ).merge(map_df).url_host_idx.values\n",
    "                           ]\n",
    "        ).\\\n",
    "        dot(\n",
    "            html_svd\n",
    "        )\n",
    "\n",
    "    htmlmap = Normalizer().fit_transform(htmlmap)\n",
    "    htmlmap = np.float32(htmlmap)\n",
    "\n",
    "    print(htmlmap.shape)\n",
    "\n",
    "    with gzip.open('auxilary/html_feats.pickle.gz', 'wb') as f:\n",
    "        pickle.dump(htmlmap, f, protocol=-1)\n",
    "\n",
    "with gzip.open('auxilary/html_feats.pickle.gz', 'rb') as f:\n",
    "    htmlmap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков SimilarWeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('auxilary/simweb_domain.pickle.gz', 'rb') as f:\n",
    "    simweb_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков Bigram ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('auxilary/bigrams_dense.pickle.gz', 'rb') as f:\n",
    "    bigrams_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение частотных ссылок мешка слов url_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20144"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_mask = (np.array((datamap['data']>0).sum(axis=0)).flatten() > 40)\n",
    "feats_mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считывание файла с таргетами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>is_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>216374</td>\n",
       "      <td>84565</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273283</td>\n",
       "      <td>239794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331154</td>\n",
       "      <td>400472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2211</td>\n",
       "      <td>22680</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369397</td>\n",
       "      <td>369554</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45304</td>\n",
       "      <td>36231</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347938</td>\n",
       "      <td>153309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247442</td>\n",
       "      <td>395326</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49156</td>\n",
       "      <td>75103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184430</td>\n",
       "      <td>179214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id   age  is_male\n",
       "216374    84565  32.0      1.0\n",
       "273283   239794   NaN      NaN\n",
       "331154   400472   NaN      NaN\n",
       "2211      22680  36.0      0.0\n",
       "369397   369554  22.0      0.0\n",
       "45304     36231  35.0      1.0\n",
       "347938   153309   NaN      NaN\n",
       "247442   395326  34.0      1.0\n",
       "49156     75103   NaN      NaN\n",
       "184430   179214   NaN      NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_df = pd.read_csv('target.tsv.gz', sep='\\t')\n",
    "trg_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка таргетов и поднабора юзеров из обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: 270000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 6, (270000, 20144))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'data'\n",
    "\n",
    "(trg_df.age.isna()|trg_df.is_male.isna()).sum(),\\\n",
    "\n",
    "all_mask = (~trg_df.age.isna()|~trg_df.is_male.isna()).values.copy()\n",
    "trg_train = trg_df[all_mask].fillna({'is_male': 0.5, 'age':34})\n",
    "trg_age = trg_train.age.values.copy()\n",
    "trg_sex = trg_train.is_male.values.copy()\n",
    "\n",
    "X_tr = datamap[key][all_mask][:, feats_mask]\n",
    "\n",
    "age_bins = [[0, 25], [26, 35], [36, 45], [46, 55], [56, 65], [66, 999]]\n",
    "\n",
    "print('Train sample:', all_mask.sum())\n",
    "\n",
    "y_all = 0\n",
    "for k, age_bin in enumerate(age_bins):\n",
    "    y = pd.Series(trg_age).between(*age_bin).values.copy()\n",
    "    y_all += y*(k+1)\n",
    "y_all.min(), y_all.max(), X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объединение мешков слов второстепенных доменов в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270000, 1407)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbag_all = hstack([regionmap,\n",
    "                   citmap,\n",
    "                   cpemanmap,\n",
    "                   cpemodnamemap,\n",
    "                   cpetypemap,\n",
    "                   podmap,\n",
    "                   datemap,\n",
    "                   pricemap_id\n",
    "                  ])\n",
    "cbag_all = QuantileTransformer(n_quantiles=10).fit_transform(cbag_all)\n",
    "cbag_all = csr_matrix(cbag_all)\n",
    "cbag_all_train = cbag_all[all_mask]\n",
    "cbag_all_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переобозначения обучающих поднаборов признаков ради удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270000, 512)\n",
      "(270000, 768)\n",
      "(270000, 13118)\n",
      "(270000, 256)\n",
      "(270000, 199)\n",
      "(270000, 512)\n"
     ]
    }
   ],
   "source": [
    "doc2vec_feats_train = doc2vec_feats[all_mask].copy()\n",
    "print(doc2vec_feats_train.shape)\n",
    "clipmap_train = clipmap[all_mask].copy()\n",
    "print(clipmap_train.shape)\n",
    "titlemap = csr_matrix(titlemap, dtype=np.float32)\n",
    "titlemap_train = titlemap[all_mask]\n",
    "print(titlemap_train.shape)\n",
    "htmlmap_train = htmlmap[all_mask].copy()\n",
    "print(htmlmap_train.shape)\n",
    "simweb_feats_train = simweb_feats[all_mask].copy()\n",
    "print(simweb_feats_train.shape)\n",
    "bigrams_feats_train = bigrams_feats[all_mask].copy()\n",
    "print(bigrams_feats_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка мешка слов ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datamap[key][:, feats_mask]\n",
    "all_data_sqrt = csr_matrix((all_data.data**0.5, all_data.nonzero()),\n",
    "                     shape=all_data.shape,\n",
    "                     dtype=np.float32)\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настройка валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10,\n",
    "                        shuffle=True,\n",
    "                        random_state=42)\n",
    "folds = [(train_ind, test_ind) for train_ind, test_ind in\n",
    "         kfold.split((np.uint8(trg_sex*2)+y_all*10).astype(str),\n",
    "                     (np.uint8(trg_sex*2)+y_all*10).astype(str))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция с архитектурой модели (последняя версия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_all(emb_size=8, dense_size=512, l1_reg=1e-7,\n",
    "                  base_lr=2e-4, sex_weight=1, age_weight=1,\n",
    "                  **kwargs):\n",
    "    l1reg = tf.keras.regularizers.l1(l1_reg)\n",
    "\n",
    "    # urls\n",
    "    inp = tf.keras.layers.Input((X_tr.shape[1],), sparse=False)\n",
    "    x = inp\n",
    "    x = tf.keras.layers.Dense(dense_size, activation='relu',\n",
    "                              use_bias=False,\n",
    "                              kernel_regularizer=l1reg)(x)\n",
    "\n",
    "    use_feats = kwargs['use_feats']\n",
    "    activation = kwargs.get('use_emb_act', 'linear')\n",
    "\n",
    "    # region id\n",
    "    inp2 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x2 = tf.keras.layers.Embedding(reg_id.max()+1,\n",
    "                                   kwargs.get('e_region', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation(activation)(x2)\n",
    "\n",
    "    # city id\n",
    "    inp3 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x3 = tf.keras.layers.Embedding(city_id.max()+1,\n",
    "                                   kwargs.get('e_city', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Activation(activation)(x3)\n",
    "\n",
    "    # cpeman id\n",
    "    inp4 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x4 = tf.keras.layers.Embedding(cpeman_id.max()+1,\n",
    "                                   kwargs.get('e_cpeman', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp4)\n",
    "    x4 = tf.keras.layers.Flatten()(x4)\n",
    "    x4 = tf.keras.layers.Activation(activation)(x4)\n",
    "\n",
    "    # cpemodname id\n",
    "    inp5 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x5 = tf.keras.layers.Embedding(cpemodname_id.max()+1,\n",
    "                                   kwargs.get('e_cpemodname', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp5)\n",
    "    x5 = tf.keras.layers.Flatten()(x5)\n",
    "    x5 = tf.keras.layers.Activation(activation)(x5)\n",
    "\n",
    "    # cpetype_id id\n",
    "    inp6 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x6 = tf.keras.layers.Embedding(cpetype_id.max()+1,\n",
    "                                   kwargs.get('e_cpetype', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp6)\n",
    "    x6 = tf.keras.layers.Flatten()(x6)\n",
    "    x6 = tf.keras.layers.Activation(activation)(x6)\n",
    "\n",
    "    # price id\n",
    "    inp7 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x7 = tf.keras.layers.Dense(kwargs.get('e_price', emb_size),\n",
    "                               activation='tanh',\n",
    "                               kernel_regularizer=l1reg)(inp7)\n",
    "\n",
    "    # cbagmap\n",
    "    inp8 = tf.keras.layers.Input((cbag_all.shape[-1],), sparse=False)\n",
    "    x8 = tf.keras.layers.Dense(kwargs.get('e_cbag', emb_size*2),\n",
    "                               activation='relu',\n",
    "                               kernel_regularizer=l1reg)(inp8)\n",
    "\n",
    "    # date_id id\n",
    "    inp9 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x9 = tf.keras.layers.Embedding(date_id.max()+1,\n",
    "                                   kwargs.get('e_date', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp9)\n",
    "    x9 = tf.keras.layers.Flatten()(x9)\n",
    "    x9 = tf.keras.layers.Activation(activation)(x9)\n",
    "\n",
    "    # pod_id id\n",
    "    inp10 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x10 = tf.keras.layers.Embedding(pod_id.max()+1,\n",
    "                                    kwargs.get('e_pod', emb_size),\n",
    "                                    embeddings_regularizer=l1reg)(inp10)\n",
    "    x10 = tf.keras.layers.Flatten()(x10)\n",
    "    x10 = tf.keras.layers.Activation(activation)(x10)\n",
    "\n",
    "    # d2v\n",
    "    inp13 = tf.keras.layers.Input((doc2vec_feats.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_doc', emb_size) == -1:\n",
    "        x13 = inp13\n",
    "    else:\n",
    "        x13 = tf.keras.layers.Dense(kwargs.get('e_doc', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp13)\n",
    "\n",
    "    # clip\n",
    "    inp14 = tf.keras.layers.Input((clipmap_train.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_clip', emb_size) == -1:\n",
    "        x14 = inp14\n",
    "    else:\n",
    "        x14 = tf.keras.layers.Dense(kwargs.get('e_clip', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp14)\n",
    "\n",
    "\n",
    "    # titles\n",
    "    inp15 = tf.keras.layers.Input((titlemap_train.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_title', emb_size*2) == -1:\n",
    "        x15 = inp15\n",
    "    else:\n",
    "        x15 = tf.keras.layers.Dense(kwargs.get('e_title', emb_size*2),\n",
    "                                    activation='relu',\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=l1reg)(inp15)\n",
    "\n",
    "    # html svd\n",
    "    inp16 = tf.keras.layers.Input((htmlmap_train.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_html', emb_size) == -1:\n",
    "        x16 = inp16\n",
    "    else:\n",
    "        x16 = tf.keras.layers.Dense(kwargs.get('e_html', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp16)\n",
    "\n",
    "    # simweb\n",
    "    inp17 = tf.keras.layers.Input((simweb_feats.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_simweb', emb_size) == -1:\n",
    "        x17 = inp17\n",
    "    else:\n",
    "        x17 = tf.keras.layers.Dense(kwargs.get('e_simweb', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp17)\n",
    "\n",
    "    # bigrams\n",
    "    inp18 = tf.keras.layers.Input((bigrams_feats.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_bigram', emb_size) == -1:\n",
    "        x18 = inp18\n",
    "    else:\n",
    "        x18 = tf.keras.layers.Dense(kwargs.get('e_bigram', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp18)\n",
    "\n",
    "    x_extra = [\n",
    "                 x2,\n",
    "                 x3,\n",
    "                 x4,\n",
    "                 x5,\n",
    "                 x6,\n",
    "                 x7,\n",
    "                 x8,\n",
    "                 x9,\n",
    "                 x10,\n",
    "                 x13,\n",
    "                 x14,\n",
    "                 x15,\n",
    "                 x16,\n",
    "                 x17,\n",
    "                 x18,\n",
    "            ]\n",
    "\n",
    "    x_extra = [xx for xx, remain in zip(x_extra, use_feats) if remain]\n",
    "\n",
    "    x_sex0 = tf.keras.layers.concatenate([x] + x_extra)\n",
    "    if kwargs.get('pre_bn', False):\n",
    "        x_sex0 = tf.keras.layers.BatchNormalization(\n",
    "            epsilon=1e-5, momentum=0.1)(x_sex0)\n",
    "    if kwargs.get('pre_dropout', False):\n",
    "        x_sex0 = tf.keras.layers.Dropout(0.1)(x_sex0)\n",
    "    x_age0 = x_sex0\n",
    "\n",
    "\n",
    "    parallel_age = []\n",
    "\n",
    "    nn_act = kwargs.get('nn_act', 'relu')\n",
    "    for _ in range(1):\n",
    "        prev_x_age = [x_age0]\n",
    "        x_age = x_age0\n",
    "        for _ in range(kwargs.get('dense_con_num', 2)):\n",
    "            x2 = tf.keras.layers.Dense(x_age.shape[-1], activation=nn_act,\n",
    "                                       use_bias=True,\n",
    "                                       kernel_regularizer=l1reg)(x_age)\n",
    "            # dense connections\n",
    "            prev_x_age.append(x2)\n",
    "            x_age = tf.keras.layers.add(prev_x_age)\n",
    "            if kwargs.get('bn', False):\n",
    "                x_age = tf.keras.layers.BatchNormalization(\n",
    "                    epsilon=1e-5, momentum=0.1)(x_age)\n",
    "            if kwargs.get('dropout', False)>0:\n",
    "                x_age = tf.keras.layers.Dropout(kwargs.get('dropout'))(x_age)\n",
    "        parallel_age.append(x_age)\n",
    "\n",
    "    if kwargs.get('age_extra_dim', False):\n",
    "        x_age = tf.keras.layers.concatenate([\n",
    "            tf.keras.layers.Dense(kwargs.get('age_extra_dim'),\n",
    "                                  activation=nn_act,\n",
    "                                  use_bias=True,\n",
    "                                  kernel_regularizer=l1reg)(x_age0)\n",
    "        ] + parallel_age)\n",
    "    else:\n",
    "        x_age = parallel_age[0]\n",
    "\n",
    "    if kwargs.get('sex_extra_dim', False):\n",
    "        x_sex = tf.keras.layers.concatenate([\n",
    "            tf.keras.layers.Dense(kwargs.get('sex_extra_dim'),\n",
    "                                  activation=nn_act,\n",
    "                                  use_bias=True,\n",
    "                                  kernel_regularizer=l1reg)(x_sex0)\n",
    "        ] + parallel_age)\n",
    "    else:\n",
    "        x_sex = parallel_age[0]\n",
    "\n",
    "    out1 = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True, name='sex',\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l1(l1_reg))(x_sex)\n",
    "\n",
    "    out2 = tf.keras.layers.Dense(6, activation='softmax', use_bias=True, name='age',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1(l1_reg))(x_age)\n",
    "\n",
    "    inps_extra = [\n",
    "        inp2,\n",
    "        inp3,\n",
    "        inp4,\n",
    "        inp5,\n",
    "        inp6,\n",
    "        inp7,\n",
    "        inp8,\n",
    "        inp9,\n",
    "        inp10,\n",
    "        inp13,\n",
    "        inp14,\n",
    "        inp15,\n",
    "        inp16,\n",
    "        inp17,\n",
    "        inp18\n",
    "    ]\n",
    "    inps_extra = [xx for xx, remain in zip(inps_extra, use_feats) if remain]\n",
    "\n",
    "    model = tf.keras.models.Model([inp] + inps_extra, [out1, out2])\n",
    "\n",
    "    max_weight = max(sex_weight, age_weight)\n",
    "    model.compile(loss={'sex':'binary_crossentropy',\n",
    "                        'age':'categorical_crossentropy'},\n",
    "                  loss_weights={'sex':sex_weight/max_weight,\n",
    "                                'age':age_weight/max_weight},\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=base_lr,\n",
    "                                                     clipvalue=kwargs.get('clipvalue', 2.)),\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_scheduler(base_lr=2e-4, factor=1., offset=0.5):\n",
    "    def scheduler(epoch, lr):\n",
    "        return base_lr*10**(-epoch*factor+offset)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генератор фолдов кроссвалидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = csr_matrix((X_tr.data**0.5, X_tr.nonzero()),\n",
    "                    shape=X_tr.shape,\n",
    "                    dtype=np.float32)\n",
    "\n",
    "y_ohe_age = np.zeros((y_all.size, y_all.max()))\n",
    "y_ohe_age[np.arange(y_all.size), y_all-1] = 1.\n",
    "\n",
    "def generate_folds(folds):\n",
    "    for k, (train_ind, test_ind) in enumerate(tqdm(folds)):\n",
    "        train_dat = X_train[train_ind]\n",
    "        train_y_sex = trg_sex[train_ind]\n",
    "        train_y_age = y_ohe_age[train_ind]\n",
    "        val_dat = X_train[test_ind]\n",
    "        val_y_sex = trg_sex[test_ind]\n",
    "        val_y_age = y_ohe_age[test_ind]\n",
    "\n",
    "        train_aux_dat = [train_dat,\n",
    "                         reg_id[all_mask][train_ind, None],\n",
    "                         city_id[all_mask][train_ind, None],\n",
    "                         cpeman_id[all_mask][train_ind, None],\n",
    "                         cpemodname_id[all_mask][train_ind, None],\n",
    "                         cpetype_id[all_mask][train_ind, None],\n",
    "                         price_id[all_mask][train_ind, None]**0.5,\n",
    "                         cbag_all_train[train_ind],\n",
    "                         date_id[all_mask][train_ind, None],\n",
    "                         pod_id[all_mask][train_ind, None],\n",
    "                         doc2vec_feats_train[train_ind],\n",
    "                         clipmap_train[train_ind],\n",
    "                         titlemap_train[train_ind],\n",
    "                         htmlmap_train[train_ind],\n",
    "                         simweb_feats_train[train_ind],\n",
    "                         bigrams_feats_train[train_ind],\n",
    "                        ]\n",
    "\n",
    "        val_aux_dat = [val_dat,\n",
    "                       reg_id[all_mask][test_ind, None],\n",
    "                       city_id[all_mask][test_ind, None],\n",
    "                       cpeman_id[all_mask][test_ind, None],\n",
    "                       cpemodname_id[all_mask][test_ind, None],\n",
    "                       cpetype_id[all_mask][test_ind, None],\n",
    "                       price_id[all_mask][test_ind, None]**0.5,\n",
    "                       cbag_all_train[test_ind],\n",
    "                       date_id[all_mask][test_ind, None],\n",
    "                       pod_id[all_mask][test_ind, None],\n",
    "                       doc2vec_feats_train[test_ind],\n",
    "                       clipmap_train[test_ind],\n",
    "                       titlemap_train[test_ind],\n",
    "                       htmlmap_train[test_ind],\n",
    "                       simweb_feats_train[test_ind],\n",
    "                       bigrams_feats_train[test_ind],\n",
    "                      ]\n",
    "\n",
    "\n",
    "        '''\n",
    "        model_lr = \\\n",
    "        Pipeline([('tfidf', TfidfTransformer(sublinear_tf=True, norm='l2')),\n",
    "                  ('model', LogisticRegression(C=1.5,\n",
    "                                               penalty='l1',\n",
    "                                               solver='liblinear',\n",
    "                                               #class_weight='balanced',\n",
    "                                               max_iter=5,\n",
    "                                               dual=False)\n",
    "                    )])\n",
    "        scores_lr = cross_val_predict(model_lr,\n",
    "                                      train_dat,\n",
    "                                      train_y_age.argmax(axis=1),\n",
    "                                      cv=5,\n",
    "                                      method='predict_proba',\n",
    "                                      n_jobs=5)\n",
    "\n",
    "        alpha = 0.1\n",
    "        train_y_age = train_y_age*(1-alpha)+scores_lr*alpha\n",
    "        '''\n",
    "\n",
    "        with gzip.open('oof_scores_260223/42/%d.pickle.gz'%(k+1), 'rb') as f:\n",
    "            oof_scores = pickle.load(f)\n",
    "\n",
    "        yield [[train_aux_dat, [train_y_sex, train_y_age,\n",
    "                                oof_scores['sex'], oof_scores['age']]],\n",
    "               [val_aux_dat, [val_y_sex, val_y_age]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мапка признаков для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat2idx_map = dict(e_region=0,\n",
    "                    e_city=1,\n",
    "                    e_cpeman=2,\n",
    "                    e_cpemodname=3,\n",
    "                    e_cpetype=4,\n",
    "                    e_price=5,\n",
    "                    e_date=6,\n",
    "                    e_pod=7,\n",
    "                    e_cbag=8,\n",
    "                    e_doc=9,\n",
    "                    e_clip=10,\n",
    "                    e_title=11,\n",
    "                    e_html=12,\n",
    "                    e_simweb=13,\n",
    "                    e_bigram=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция для получения скоров ансамбля моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_state(model, mode='train'):\n",
    "    for i, l in enumerate(model.layers):\n",
    "        model.layers[i].trainable = (mode=='train')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_score(args):\n",
    "    # delete useless values\n",
    "    for k, v in args.copy().items():\n",
    "        if v is None:\n",
    "            del args[k]\n",
    "            continue\n",
    "        if k.startswith('e_') or 'extra' in k:\n",
    "            args[k] = int(args[k])\n",
    "\n",
    "    use_feats = [1 for _ in range(len(args.get('use_feats', feat2idx_map)))]\n",
    "    for k, v in feat2idx_map.items():\n",
    "        if v >= len(use_feats): break\n",
    "        use_feats[v] = int(args.get(k, 100) != 1)\n",
    "\n",
    "    args['use_feats'] = tuple(use_feats)\n",
    "\n",
    "    epochs = args['epochs'] = int(args['epochs']) #4\n",
    "    batch_size = args['batch_size'] = int(args['batch_size']) #128+256\n",
    "    steps_subsample = args['steps_subsample'] = float('%.3f'%args['steps_subsample']) #0.95\n",
    "    base_lr = args['base_lr'] = args['base_lr'] #2e-4\n",
    "    factor = args['factor'] = float('%.3f'%args['factor']) #1.\n",
    "    offset = args['offset'] = float('%.3f'%args['offset']) #0.5\n",
    "    emb_size = args['emb_size'] = int(args['emb_size']) # 64\n",
    "    dense_size = args['dense_size'] = int(args['dense_size']) #1024\n",
    "    sex_weight = args['sex_weight'] = float('%.3f'%args['sex_weight'])\n",
    "    age_weight = args['age_weight'] = float('%.3f'%args['age_weight'])\n",
    "    sex_alpha = args['sex_alpha']\n",
    "    age_alpha = args['age_alpha']\n",
    "    scheduler = get_scheduler(base_lr, factor, offset)\n",
    "\n",
    "    AUX_DAT = [\n",
    "             all_data_sqrt,\n",
    "             reg_id[:, None],\n",
    "             city_id[:, None],\n",
    "             cpeman_id[:, None],\n",
    "             cpemodname_id[:, None],\n",
    "             cpetype_id[:, None],\n",
    "             price_id[:, None]**0.5,\n",
    "             cbag_all,\n",
    "             date_id[:, None],\n",
    "             pod_id[:, None],\n",
    "             doc2vec_feats,\n",
    "             clipmap,\n",
    "             titlemap,\n",
    "             htmlmap,\n",
    "             simweb_feats,\n",
    "             bigrams_feats,\n",
    "    ]\n",
    "    AUX_DAT = [xx for xx, remain in zip(AUX_DAT, [1]+use_feats) if remain]\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    model_weights = []\n",
    "    preds_all = []\n",
    "\n",
    "    scores = []\n",
    "    for cache, (train_ind, test_ind) in zip(generate_folds(folds), tqdm(folds)):\n",
    "        [train_aux_dat, train_y],\\\n",
    "        [val_aux_dat, val_y] = cache\n",
    "        train_y_sex, train_y_age, oof_sex, oof_age = train_y\n",
    "        val_y_sex, val_y_age = val_y\n",
    "\n",
    "        train_aux_dat = [xx for xx, remain in zip(train_aux_dat, [1]+use_feats) if remain]\n",
    "        val_aux_dat = [xx for xx, remain in zip(val_aux_dat, [1]+use_feats) if remain]\n",
    "\n",
    "        for _ in range(3):\n",
    "            model_nn = get_model_all(**args)\n",
    "\n",
    "            st_time = time.time()\n",
    "\n",
    "            weights_backup = model_nn.get_weights()\n",
    "\n",
    "            model_nn.fit(train_aux_dat,\n",
    "                      [(oof_sex*sex_alpha + train_y_sex*(1 - sex_alpha)),\n",
    "                       (oof_age*age_alpha + train_y_age*(1 - age_alpha))],\n",
    "                      batch_size=batch_size,\n",
    "                      steps_per_epoch=int(steps_subsample*train_ind.size/batch_size),\n",
    "                      epochs=epochs,\n",
    "                      callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler)],\n",
    "                      verbose=True)\n",
    "\n",
    "            if args.get('pseudo', False):\n",
    "                # PSEUDO-labelling\n",
    "                model_nn = change_state(model_nn, 'test')\n",
    "                AUX_PREDS = model_nn.predict(AUX_DAT, batch_size=1024)\n",
    "                model_nn = change_state(model_nn, 'train')\n",
    "                model_nn.set_weights(weights_backup)\n",
    "                del weights_backup\n",
    "\n",
    "                tf.keras.backend.set_value(model_nn.optimizer.lr, args.get('pretrain_lr', base_lr))\n",
    "                model_nn.fit(AUX_DAT, AUX_PREDS,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=1,\n",
    "                          verbose=False)\n",
    "\n",
    "                tf.keras.backend.set_value(model_nn.optimizer.lr, base_lr)\n",
    "                model_nn.fit(train_aux_dat,\n",
    "                             [oof_sex*sex_alpha + train_y_sex*(1 - sex_alpha),\n",
    "                              oof_age*age_alpha + train_y_age*(1 - age_alpha)],\n",
    "                          batch_size=batch_size,\n",
    "                          steps_per_epoch=int(steps_subsample*train_ind.size/batch_size),\n",
    "                          epochs=epochs,\n",
    "                          callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler)],\n",
    "                          verbose=False)\n",
    "\n",
    "            model_nn = change_state(model_nn, 'test')\n",
    "            preds = \\\n",
    "            model_nn.predict(val_aux_dat, batch_size=1024)\n",
    "\n",
    "            preds_all.append(preds)\n",
    "\n",
    "            model_weights.append(model_nn.get_weights())\n",
    "\n",
    "            end_time = time.time()\n",
    "            spent_seconds = end_time - st_time\n",
    "\n",
    "            f1 = \\\n",
    "            f1_score(val_y_age.argmax(axis=1),\n",
    "                     preds[-1].argmax(axis=1), average='weighted')*100\n",
    "            rocauc = \\\n",
    "            roc_auc_score(val_y_sex[val_y_sex!=0.5],\n",
    "                          preds[0].flatten()[val_y_sex!=0.5])*100\n",
    "\n",
    "            score = f1*2 + (rocauc-50)*2\n",
    "            scores.append((score, f1, rocauc, spent_seconds))\n",
    "\n",
    "            print(*scores[-1], sep='\\t')\n",
    "    return model_weights, preds_all, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример набора конфигураций архитектур моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-03-29\n",
    "model_argses = [\n",
    "    {'age_alpha': 0.7000000000000001,\n",
    "    'age_extra_dim': 32,\n",
    "    'age_weight': 4.2,\n",
    "    'base_lr': 0.0017782794100389228,\n",
    "    'batch_size': 256,\n",
    "    'bn': True,\n",
    "    'clipvalue': 1.0,\n",
    "    'dense_size': 608,\n",
    "    'dropout': 0.05,\n",
    "    'e_bigram': 107,\n",
    "    'e_cbag': 181,\n",
    "    'e_city': 90,\n",
    "    'e_cpeman': 38,\n",
    "    'e_cpemodname': 1,\n",
    "    'e_cpetype': 90,\n",
    "    'e_doc': -1,\n",
    "    'e_html': -1,\n",
    "    'e_pod': 64,\n",
    "    'e_region': 90,\n",
    "    'emb_size': 76,\n",
    "    'epochs': 4,\n",
    "    'factor': 1.0,\n",
    "    'l1_reg': 3.162277660168379e-07,\n",
    "    'nn_act': 'elu',\n",
    "    'offset': 0.2,\n",
    "    'pre_dropout': True,\n",
    "    'pretrain_lr': 0.0001,\n",
    "    'pseudo': False,\n",
    "    'sex_alpha': 0.55,\n",
    "    'sex_weight': 2.2,\n",
    "    'steps_subsample': 1.0,\n",
    "    'use_emb_act': 'tanh',\n",
    "    'use_feats': (1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)},\n",
    "    {'age_alpha': 0.7000000000000001,\n",
    "    'age_extra_dim': 32,\n",
    "    'age_weight': 3.9,\n",
    "    'base_lr': 0.0031622776601683794,\n",
    "    'batch_size': 256,\n",
    "    'bn': True,\n",
    "    'clipvalue': 1.0,\n",
    "    'dense_size': 861,\n",
    "    'dropout': 0.05,\n",
    "    'e_bigram': 128,\n",
    "    'e_cbag': 304,\n",
    "    'e_city': 152,\n",
    "    'e_cpeman': 19,\n",
    "    'e_cpemodname': 1,\n",
    "    'e_cpetype': 32,\n",
    "    'e_doc': -1,\n",
    "    'e_html': -1,\n",
    "    'e_pod': 53,\n",
    "    'e_region': 53,\n",
    "    'emb_size': 64,\n",
    "    'epochs': 4,\n",
    "    'factor': 1.1,\n",
    "    'l1_reg': 1e-07,\n",
    "    'nn_act': 'elu',\n",
    "    'offset': 0.0,\n",
    "    'pre_dropout': True,\n",
    "    'pretrain_lr': 0.00031622776601683794,\n",
    "    'pseudo': False,\n",
    "    'sex_alpha': 0.75,\n",
    "    'sex_weight': 2.2,\n",
    "    'steps_subsample': 1.0,\n",
    "    'use_emb_act': 'tanh',\n",
    "    'use_feats': (1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция для подбора оптимального смещения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(x, *args):\n",
    "    x, s = x[:len(age_bins)], scipy_s\n",
    "    x = np.maximum(x, scipy_eps)\n",
    "    x /= x.sum()\n",
    "    return -f1_score(y_all-1,\n",
    "                     (preds_age_logits+x*s).argmax(axis=1),\n",
    "                     average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение предсказаний от ансамбля нейронок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'age_alpha': 0.2, 'age_extra_dim': 256, 'age_weight': 2.6, 'base_lr': 5.623413251903491e-05, 'batch_size': 192, 'bn': True, 'clipvalue': 4.0, 'dense_con_num': 3, 'dense_size': 1024, 'e_cbag': 32, 'e_city': 1, 'e_clip': 1, 'e_cpemodname': 9, 'e_cpetype': 1, 'e_pod': 1, 'e_price': 1, 'e_simweb': -1, 'e_title': 1, 'emb_size': 13, 'epochs': 4, 'factor': 1.4, 'l1_reg': 3.162277660168379e-07, 'nn_act': 'elu', 'offset': 1.0, 'pretrain_lr': 0.0005623413251903491, 'pseudo': True, 'sex_alpha': 0.35000000000000003, 'sex_weight': 2.1, 'steps_subsample': 0.95, 'use_feats': (1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4828b545faa24273955258803a6b0f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3d4c87bd3e4ddf917170cadd47df92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_390/dense_4260/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_390/dense_4260/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_390/dense_4260/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_390/dense_4262/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_390/dense_4262/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_390/dense_4262/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 33s 22ms/step - loss: 1.7578 - sex_loss: 0.4959 - age_loss: 1.2818\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 29s 22ms/step - loss: 1.5889 - sex_loss: 0.4563 - age_loss: 1.1471\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 29s 23ms/step - loss: 1.5694 - sex_loss: 0.4529 - age_loss: 1.1312\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 29s 23ms/step - loss: 1.5689 - sex_loss: 0.4524 - age_loss: 1.1311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0670936972474958a6c859b1a03931b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.97472761871808\t48.39572427590716\t88.59163953345188\t350.71255445480347\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_391/dense_4272/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_391/dense_4272/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_391/dense_4272/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_391/dense_4274/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_391/dense_4274/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_391/dense_4274/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 32s 23ms/step - loss: 1.7549 - sex_loss: 0.4947 - age_loss: 1.2794\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 29s 22ms/step - loss: 1.5920 - sex_loss: 0.4578 - age_loss: 1.1485\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 29s 22ms/step - loss: 1.5736 - sex_loss: 0.4542 - age_loss: 1.1340\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 31s 24ms/step - loss: 1.5726 - sex_loss: 0.4541 - age_loss: 1.1332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557f9fadf77142f0b3c181ca765e7467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.1719040565693\t48.47452400504317\t88.61142802324149\t390.9869201183319\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_392/dense_4284/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_392/dense_4284/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_392/dense_4284/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_392/dense_4286/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_392/dense_4286/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_392/dense_4286/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 38s 25ms/step - loss: 1.7592 - sex_loss: 0.4981 - age_loss: 1.2807\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 33s 25ms/step - loss: 1.5900 - sex_loss: 0.4563 - age_loss: 1.1475\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 33s 25ms/step - loss: 1.5717 - sex_loss: 0.4533 - age_loss: 1.1326\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 33s 25ms/step - loss: 1.5701 - sex_loss: 0.4531 - age_loss: 1.1312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f36e1d1fa3149878f165637d282ab53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.09780585947487\t48.50420629196513\t88.5446966377723\t407.76906752586365\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_393/dense_4296/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_393/dense_4296/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_393/dense_4296/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_393/dense_4298/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_393/dense_4298/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_393/dense_4298/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 37s 25ms/step - loss: 1.7560 - sex_loss: 0.4958 - age_loss: 1.2805\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 33s 25ms/step - loss: 1.5903 - sex_loss: 0.4577 - age_loss: 1.1480\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 33s 25ms/step - loss: 1.5719 - sex_loss: 0.4540 - age_loss: 1.1334\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 32s 25ms/step - loss: 1.5709 - sex_loss: 0.4540 - age_loss: 1.1325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fa1325b8d243c18ab05c61ed70dbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.1971953611388\t48.235646273810076\t88.86295140675932\t403.336975812912\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_394/dense_4308/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_394/dense_4308/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_394/dense_4308/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_394/dense_4310/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_394/dense_4310/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_394/dense_4310/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 38s 25ms/step - loss: 1.7562 - sex_loss: 0.4958 - age_loss: 1.2811\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 32s 25ms/step - loss: 1.5890 - sex_loss: 0.4574 - age_loss: 1.1469\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 32s 25ms/step - loss: 1.5709 - sex_loss: 0.4537 - age_loss: 1.1327\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 33s 25ms/step - loss: 1.5695 - sex_loss: 0.4536 - age_loss: 1.1313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b622e2ad70a43eba2c1a1afa67d3221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.23362795552833\t48.230154464893396\t88.88665951287076\t399.49020886421204\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_395/dense_4320/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_395/dense_4320/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_395/dense_4320/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_395/dense_4322/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_395/dense_4322/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_395/dense_4322/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 37s 25ms/step - loss: 1.7530 - sex_loss: 0.4960 - age_loss: 1.2765\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 32s 25ms/step - loss: 1.5904 - sex_loss: 0.4573 - age_loss: 1.1477\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 32s 25ms/step - loss: 1.5730 - sex_loss: 0.4539 - age_loss: 1.1341\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 32s 25ms/step - loss: 1.5724 - sex_loss: 0.4536 - age_loss: 1.1337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043dd0a7206148a8b700822e931f2e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.43053957288572\t47.89506559882889\t88.82020418761397\t394.3228905200958\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_396/dense_4332/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_396/dense_4332/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_396/dense_4332/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_396/dense_4334/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_396/dense_4334/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_396/dense_4334/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 36s 25ms/step - loss: 1.7561 - sex_loss: 0.4970 - age_loss: 1.2798\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 32s 24ms/step - loss: 1.5890 - sex_loss: 0.4577 - age_loss: 1.1468\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 32s 24ms/step - loss: 1.5712 - sex_loss: 0.4541 - age_loss: 1.1327\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 32s 24ms/step - loss: 1.5702 - sex_loss: 0.4538 - age_loss: 1.1321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a720655eef54c4e9d0352db8ed4df9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.00067615636695\t48.19739728479435\t88.80294079338913\t381.6654200553894\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_397/dense_4344/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_397/dense_4344/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_397/dense_4344/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_397/dense_4346/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_397/dense_4346/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_397/dense_4346/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 37s 24ms/step - loss: 1.7579 - sex_loss: 0.4976 - age_loss: 1.2812\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 31s 24ms/step - loss: 1.5894 - sex_loss: 0.4565 - age_loss: 1.1480\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 30s 23ms/step - loss: 1.5709 - sex_loss: 0.4529 - age_loss: 1.1332\n",
      "Epoch 4/4\n",
      "1202/1202 [==============================] - 30s 23ms/step - loss: 1.5698 - sex_loss: 0.4528 - age_loss: 1.1323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8e86ff00264f01aa115522d0068e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.74316629959458\t48.02121147071683\t88.85037167908045\t356.58821058273315\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_398/dense_4356/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_398/dense_4356/embedding_lookup_sparse/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_398/dense_4356/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_398/dense_4358/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_398/dense_4358/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_398/dense_4358/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202/1202 [==============================] - 32s 23ms/step - loss: 1.7570 - sex_loss: 0.4965 - age_loss: 1.2798\n",
      "Epoch 2/4\n",
      "1202/1202 [==============================] - 29s 22ms/step - loss: 1.5910 - sex_loss: 0.4570 - age_loss: 1.1481\n",
      "Epoch 3/4\n",
      "1202/1202 [==============================] - 29s 22ms/step - loss: 1.5730 - sex_loss: 0.4534 - age_loss: 1.1340\n",
      "Epoch 4/4\n",
      " 509/1202 [===========>..................] - ETA: 15s - loss: 1.5725 - sex_loss: 0.4531 - age_loss: 1.1337"
     ]
    }
   ],
   "source": [
    "# generate preds\n",
    "savefile = 'preds_290323_age_p%d.pickle.gz'\n",
    "K = 1\n",
    "np.random.choice(42)\n",
    "order = np.random.permutation(len(model_argses))\n",
    "while True:\n",
    "    if os.path.exists(savefile%K):\n",
    "        K += 1\n",
    "        continue\n",
    "\n",
    "    model_args = model_argses[order[(K-1)%len(model_argses)]]\n",
    "    model_args['pseudo'] = K % 3 == 0\n",
    "    print(model_args['pseudo'])\n",
    "\n",
    "    model_weights, preds_all, scores = \\\n",
    "        get_score(model_args)\n",
    "\n",
    "    preds_age = np.zeros_like(y_all)\n",
    "    preds_age_logits = np.zeros((y_all.shape[0], np.unique(y_all).size))\n",
    "    preds_sex = np.zeros_like(trg_sex)\n",
    "    for i, (train_ind, test_ind) in enumerate(tqdm(folds)):\n",
    "        for k in range(len(model_weights)//len(folds)):\n",
    "            offset = len(model_weights)//len(folds)*i + k\n",
    "            preds_age_logits[test_ind] += preds_all[offset][-1]\n",
    "            preds_sex[test_ind] += (\n",
    "                preds_all[offset][0].flatten().argsort().argsort() / preds_all[offset][0].size\n",
    "            )\n",
    "        preds_age[test_ind] = preds_age_logits[test_ind].argmax(axis=1)\n",
    "    preds_age_logits /= (len(model_weights)//len(folds))\n",
    "    assert preds_age_logits.max() <= 1\n",
    "\n",
    "    print(\n",
    "        f1_score(y_all-1,\n",
    "                 preds_age,\n",
    "                 average='weighted'),\\\n",
    "        roc_auc_score(trg_sex[trg_sex!=0.5],\n",
    "                      preds_sex[trg_sex!=0.5])\n",
    "        )\n",
    "\n",
    "    bscore = f1_score(y_all-1, preds_age_logits.argmax(axis=1),\n",
    "                  average='weighted')\n",
    "    random_biases = np.random.dirichlet(np.ones(len(age_bins))/len(age_bins),\n",
    "                                        size=100)*\\\n",
    "                    np.random.random(size=(100, 1))*\\\n",
    "                    0.5\n",
    "    for random_bias in tqdm(random_biases):\n",
    "        score = \\\n",
    "        f1_score(y_all-1,\n",
    "                 (preds_age_logits+random_bias).argmax(axis=1),\n",
    "                 average='weighted')\n",
    "        if score > bscore:\n",
    "            bscore = score\n",
    "            bias = random_bias\n",
    "            print(bscore, bias)\n",
    "\n",
    "    # cyborg-ml подобрал это значение, потом уже не менял\n",
    "    scipy_s = 0.3\n",
    "    scipy_eps = 1e-10\n",
    "    scipy_res = None\n",
    "    for _ in tqdm(range(5)):\n",
    "        scipy_res = minimize(f1_weighted,\n",
    "                             np.ones(len(age_bins)) if scipy_res is None else scipy_res.x,\n",
    "                             method='COBYLA')\n",
    "        scipy_bias = np.maximum(scipy_res.x, scipy_eps)/\\\n",
    "                     np.maximum(scipy_res.x, scipy_eps).sum()*\\\n",
    "                     scipy_s\n",
    "        print(abs(scipy_res.fun), scipy_bias)\n",
    "\n",
    "    all_preds_age = 0\n",
    "    all_preds_sex = 0\n",
    "\n",
    "    use_feats = [1] + list(model_args['use_feats'])\n",
    "    all_feats = [all_data_sqrt,\n",
    "                 reg_id[:, None],\n",
    "                 city_id[:, None],\n",
    "                 cpeman_id[:, None],\n",
    "                 cpemodname_id[:, None],\n",
    "                 cpetype_id[:, None],\n",
    "                 price_id[:, None]**0.5,\n",
    "                 cbag_all,\n",
    "                 date_id[:, None],\n",
    "                 pod_id[:, None],\n",
    "                 doc2vec_feats,\n",
    "                 clipmap,\n",
    "                 titlemap,\n",
    "                 htmlmap,\n",
    "                 simweb_feats,\n",
    "                 bigrams_feats,\n",
    "                ]\n",
    "    all_feats = [xx for xx, remain in zip(all_feats, use_feats) if remain]\n",
    "    model_nn = get_model_all(**model_args)\n",
    "    model_nn = change_state(model_nn, 'test')\n",
    "\n",
    "    for ws in tqdm(model_weights):\n",
    "        model_nn.set_weights(ws)\n",
    "        preds = \\\n",
    "            model_nn.predict(all_feats, batch_size=2048)\n",
    "\n",
    "        all_preds_sex += preds[0].flatten().argsort().argsort()/all_data_sqrt.shape[0]\n",
    "        all_preds_age += preds[-1]\n",
    "        del preds\n",
    "    all_preds_sex /= len(model_weights)\n",
    "    all_preds_age /= len(model_weights)\n",
    "\n",
    "    all_preds_age += scipy_bias\n",
    "    all_preds_age_label = all_preds_age.argmax(axis=1) + 1\n",
    "\n",
    "    all_preds_sex[all_mask] = preds_sex\n",
    "    all_preds_age[all_mask] = preds_age_logits + scipy_bias\n",
    "\n",
    "    with gzip.open(savefile%K, 'wb') as f:\n",
    "        pickle.dump(dict(sex=all_preds_sex,\n",
    "                         age=all_preds_age,\n",
    "                         age_bias=scipy_bias,\n",
    "                         cv_stats=scores,\n",
    "                         model_params=model_args),\n",
    "                    f, protocol=-1)\n",
    "\n",
    "    K += 1\n",
    "\n",
    "    clear_output()\n",
    "    print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично формируются файлики скоров для пола"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
