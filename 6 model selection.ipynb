{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ограничения на ресурсы для numpy, импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\" # export NUMEXPR_NUM_THREADS=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from functools import reduce\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import QuantileTransformer, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "from multiprocessing.pool import Pool\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, atpe, Trials, STATUS_OK\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import gzip\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from functools import partial\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по регионам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 79)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/region_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "reg_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    reg_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(reg_id, return_counts=True)\n",
    "_id_map = {}\n",
    "regionmap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(regionmap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "reg_id = np.array([_id_map.get(cid, len(_id_map)) for cid in reg_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "reg_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по городам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 661)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/city_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "city_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    city_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(city_id, return_counts=True)\n",
    "_id_map = {}\n",
    "citmap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(citmap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "city_id = np.array([_id_map.get(cid, len(_id_map)) for cid in city_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "city_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по производителю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/cpe_manufacturer_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "cpeman_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    cpeman_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(cpeman_id, return_counts=True)\n",
    "_id_map = {}\n",
    "cpemanmap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(cpemanmap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "cpeman_id = np.array([_id_map.get(cid, len(_id_map)) for cid in cpeman_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "cpeman_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по устройству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 396)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/cpe_model_name_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "cpemodname_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "pd.Series(\n",
    "    cpemodname_id\n",
    ").value_counts().head(50).index\n",
    "\n",
    "_id_label, _id_cnt = np.unique(cpemodname_id, return_counts=True)\n",
    "_id_map = {}\n",
    "cpemodnamemap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(cpemodnamemap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "cpemodname_id = np.array([_id_map.get(cid, len(_id_map)) for cid in cpemodname_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "cpemodname_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по cpe_type_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 2, 1, 3], dtype='int64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/cpe_type_cd_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "cpetype_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "cpetypemap = csr_matrix(regmap['data'])\n",
    "print(cpetypemap.shape)\n",
    "pd.Series(\n",
    "    cpetype_id\n",
    ").value_counts().head(50).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по датам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 397)\n",
      "(415317, 203)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/date_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "date_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "datemap = csr_matrix(regmap['data'])\n",
    "print(datemap.shape)\n",
    "pd.Series(\n",
    "    date_id\n",
    ").value_counts().tail(50)\n",
    "\n",
    "_id_label, _id_cnt = np.unique(date_id, return_counts=True)\n",
    "_id_map = {}\n",
    "datemap = csr_matrix(regmap['data'][:, _id_label[_id_cnt>20]])\n",
    "print(datemap.shape)\n",
    "for cid in _id_label[_id_cnt>20]:\n",
    "    _id_map[cid] = len(_id_map)\n",
    "date_id = np.array([_id_map.get(cid, len(_id_map)) for cid in date_id])\n",
    "del _id_label, _id_cnt, _id_map\n",
    "date_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по времени суток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415317, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    217543\n",
       "2    126023\n",
       "0     64969\n",
       "3      6782\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/part_of_day_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "pod_id = np.array(regmap['data'].argmax(axis=1)).flatten()\n",
    "podmap = csr_matrix(regmap['data'])\n",
    "print(podmap.shape)\n",
    "pd.Series(\n",
    "    pod_id\n",
    ").value_counts().tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по ценам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('files/price_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    regmap = pickle.load(f)\n",
    "\n",
    "prices = np.array(pd.read_csv('files/price_mapper.tsv.gz', sep='\\t').price.fillna(20_000).tolist()\n",
    "                  + [20_000])\n",
    "price_id = regmap['data']\n",
    "price_id = (price_id.dot(prices)/np.array(price_id.sum(axis=1)).flatten())\n",
    "pricemap_id = regmap['data'].dot(KBinsDiscretizer(n_bins=31,\n",
    "                                                  strategy='kmeans',).fit_transform(prices[:, None]**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка мешка слов по ссылкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': <415317x199684 sparse matrix of type '<class 'numpy.uint32'>'\n",
       " \twith 32277669 stored elements in Compressed Sparse Row format>,\n",
       " 'uids': array([     4,     16,     18, ..., 415276, 415288, 415293])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('files/url_host_cbag_v2.pickle.gz', 'rb') as f:\n",
    "    datamap = pickle.load(f)\n",
    "datamap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков тайтлов ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<415317x13118 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 82784390 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('auxilary/domain20k_title.pickle.gz', 'rb') as f:\n",
    "    titlemap = pickle.load(f)\n",
    "\n",
    "map_df = pd.read_csv('auxilary/url_host_mapper_v2.tsv.gz', sep='\\t')\n",
    "\n",
    "titlemap = \\\n",
    "datamap['data'][:, pd.DataFrame(\n",
    "    dict(url_host=titlemap['domain'])\n",
    "        ).merge(map_df).url_host_idx.values\n",
    "               ].dot(CountVectorizer(ngram_range=(1,2),\n",
    "                                     min_df=2,).\\\n",
    "                     fit_transform(titlemap['title']))\n",
    "\n",
    "titlemap = csr_matrix((np.log2(1+titlemap.data), titlemap.nonzero()),\n",
    "                      shape=titlemap.shape, dtype=np.float32)\n",
    "titlemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка эмбеддингов скриншотов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(415317, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('auxilary/clipVIT_scores_20k.pickle.gz', 'rb') as f:\n",
    "    clipmap = pickle.load(f)\n",
    "\n",
    "clipmap = \\\n",
    "TfidfTransformer(sublinear_tf=True, norm=None).\\\n",
    "    fit_transform(\n",
    "        datamap['data'][:, pd.DataFrame(\n",
    "            dict(url_host=clipmap['domains'])\n",
    "                ).merge(map_df).url_host_idx.values\n",
    "                       ]\n",
    "    ).\\\n",
    "    dot(\n",
    "        np.array(clipmap['scores'])\n",
    "    )\n",
    "\n",
    "clipmap = Normalizer().fit_transform(clipmap)\n",
    "clipmap = np.float32(clipmap)\n",
    "\n",
    "clipmap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('doc2vec_feats_128x4.pickle.gz', 'rb') as f:\n",
    "    doc2vec_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 646 ms, total: 3.66 s\n",
      "Wall time: 3.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists('html_feats.pickle.gz'):\n",
    "\n",
    "\n",
    "    with gzip.open('auxilary/domain20k_html.txt.gz', 'rt',\n",
    "                   encoding='utf-8') as f:\n",
    "        html_data = f.read().split('\\n=\\n=\\n')\n",
    "        print(len(html_data))\n",
    "\n",
    "    def tokenize(x):\n",
    "        return x[0], re.findall('(?u)\\\\b\\\\w\\\\w+\\\\b', x[-1].lower())\n",
    "\n",
    "    with Pool(20) as pool:\n",
    "        pool_pbar = tqdm()\n",
    "        html_tokens = [None for _ in range(len(html_data[:-1]))]\n",
    "        for i, r in pool.imap(tokenize, enumerate(html_data[:-1])):\n",
    "            html_tokens[i] = r\n",
    "            pool_pbar.update(1)\n",
    "\n",
    "    html_cbag = \\\n",
    "    CountVectorizer(ngram_range=(1,2),\n",
    "                    min_df=300,\n",
    "                    lowercase=False,\n",
    "                    tokenizer=lambda x: x,\n",
    "                    max_df=0.25).\\\n",
    "                   fit_transform(html_tokens)\n",
    "    print(html_cbag.shape)\n",
    "\n",
    "    html_tfidf = TfidfTransformer(sublinear_tf=True).fit_transform(html_cbag)\n",
    "\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    html_svd = TruncatedSVD(n_components=256,\n",
    "                            random_state=10,\n",
    "                            n_iter=3,).fit_transform(html_tfidf)\n",
    "    print(html_svd.shape)\n",
    "\n",
    "    with gzip.open('auxilary/domain20k_title.pickle.gz', 'rb') as f:\n",
    "        htmlmap = pickle.load(f)\n",
    "    htmlmap = \\\n",
    "    TfidfTransformer(sublinear_tf=True, norm=None).\\\n",
    "        fit_transform(\n",
    "            datamap['data'][:, pd.DataFrame(\n",
    "                dict(url_host=htmlmap['domain'])\n",
    "                    ).merge(map_df).url_host_idx.values\n",
    "                           ]\n",
    "        ).\\\n",
    "        dot(\n",
    "            html_svd\n",
    "        )\n",
    "\n",
    "    htmlmap = Normalizer().fit_transform(htmlmap)\n",
    "    htmlmap = np.float32(htmlmap)\n",
    "\n",
    "    print(htmlmap.shape)\n",
    "\n",
    "    with gzip.open('auxilary/html_feats.pickle.gz', 'wb') as f:\n",
    "        pickle.dump(htmlmap, f, protocol=-1)\n",
    "\n",
    "with gzip.open('auxilary/html_feats.pickle.gz', 'rb') as f:\n",
    "    htmlmap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков SimilarWeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('auxilary/simweb_domain.pickle.gz', 'rb') as f:\n",
    "    simweb_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка признаков Bigram ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('auxilary/bigrams_dense.pickle.gz', 'rb') as f:\n",
    "    bigrams_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение частотных ссылок мешка слов url_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20144"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_mask = (np.array((datamap['data']>0).sum(axis=0)).flatten() > 40)\n",
    "feats_mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считывание файла с таргетами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>is_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>163642</td>\n",
       "      <td>389301</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208957</td>\n",
       "      <td>11227</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326661</td>\n",
       "      <td>356595</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339384</td>\n",
       "      <td>66835</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372068</td>\n",
       "      <td>395895</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367227</td>\n",
       "      <td>346685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392838</td>\n",
       "      <td>189980</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85509</td>\n",
       "      <td>23586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281517</td>\n",
       "      <td>320627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398586</td>\n",
       "      <td>248151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id   age  is_male\n",
       "163642   389301  23.0      1.0\n",
       "208957    11227  42.0      0.0\n",
       "326661   356595  41.0      0.0\n",
       "339384    66835  20.0      1.0\n",
       "372068   395895  42.0      1.0\n",
       "367227   346685   NaN      NaN\n",
       "392838   189980  41.0      1.0\n",
       "85509     23586   NaN      NaN\n",
       "281517   320627   NaN      NaN\n",
       "398586   248151   NaN      NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_df = pd.read_csv('target.tsv.gz', sep='\\t')\n",
    "trg_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка таргетов и поднабора юзеров из обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: 270000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 6, (270000, 20144))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'data'\n",
    "\n",
    "(trg_df.age.isna()|trg_df.is_male.isna()).sum(),\\\n",
    "\n",
    "all_mask = (~trg_df.age.isna()|~trg_df.is_male.isna()).values.copy()\n",
    "trg_train = trg_df[all_mask].fillna({'is_male': 0.5, 'age': 34})\n",
    "trg_age = trg_train.age.values.copy()\n",
    "trg_sex = trg_train.is_male.values.copy()\n",
    "\n",
    "X_tr = datamap[key][all_mask][:, feats_mask]\n",
    "\n",
    "age_bins = [[0, 25], [26, 35], [36, 45], [46, 55], [56, 65], [66, 999]]\n",
    "\n",
    "print('Train sample:', all_mask.sum())\n",
    "\n",
    "y_all = 0\n",
    "for k, age_bin in enumerate(age_bins):\n",
    "    y = pd.Series(trg_age).between(*age_bin).values.copy()\n",
    "    y_all += y*(k+1)\n",
    "y_all.min(), y_all.max(), X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объединение мешков слов второстепенных доменов в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270000, 1407)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbag_all = hstack([regionmap,\n",
    "                   citmap,\n",
    "                   cpemanmap,\n",
    "                   cpemodnamemap,\n",
    "                   cpetypemap,\n",
    "                   podmap,\n",
    "                   datemap,\n",
    "                   pricemap_id\n",
    "                  ])\n",
    "cbag_all = QuantileTransformer(n_quantiles=10).fit_transform(cbag_all)\n",
    "cbag_all = csr_matrix(cbag_all)\n",
    "cbag_all_train = cbag_all[all_mask]\n",
    "cbag_all_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переобозначения обучающих поднаборов признаков ради удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270000, 512)\n",
      "(270000, 768)\n",
      "(270000, 13118)\n",
      "(270000, 256)\n",
      "(270000, 199)\n",
      "(270000, 512)\n"
     ]
    }
   ],
   "source": [
    "doc2vec_feats_train = doc2vec_feats[all_mask].copy()\n",
    "print(doc2vec_feats_train.shape)\n",
    "clipmap_train = clipmap[all_mask].copy()\n",
    "print(clipmap_train.shape)\n",
    "titlemap = csr_matrix(titlemap, dtype=np.float32)\n",
    "titlemap_train = titlemap[all_mask]\n",
    "print(titlemap_train.shape)\n",
    "htmlmap_train = htmlmap[all_mask].copy()\n",
    "print(htmlmap_train.shape)\n",
    "simweb_feats_train = simweb_feats[all_mask].copy()\n",
    "print(simweb_feats_train.shape)\n",
    "bigrams_feats_train = bigrams_feats[all_mask].copy()\n",
    "print(bigrams_feats_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка мешка слов ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datamap[key][:, feats_mask]\n",
    "all_data_sqrt = csr_matrix((all_data.data**0.5, all_data.nonzero()),\n",
    "                     shape=all_data.shape,\n",
    "                     dtype=np.float32)\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настройка валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10,\n",
    "                        shuffle=True,\n",
    "                        random_state=101010)\n",
    "folds = [(train_ind, test_ind) for train_ind, test_ind in\n",
    "         kfold.split((np.uint8(trg_sex*2)+y_all*10).astype(str),\n",
    "                     (np.uint8(trg_sex*2)+y_all*10).astype(str))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция с архитектурой модели (последняя версия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_all(emb_size=8, dense_size=512, l1_reg=1e-7,\n",
    "                  base_lr=2e-4, sex_weight=1, age_weight=1,\n",
    "                  **kwargs):\n",
    "    l1reg = tf.keras.regularizers.l1(l1_reg)\n",
    "\n",
    "    # urls\n",
    "    inp = tf.keras.layers.Input((X_tr.shape[1],), sparse=False)\n",
    "    x = inp\n",
    "    x = tf.keras.layers.Dense(dense_size, activation='relu',\n",
    "                              use_bias=False,\n",
    "                              kernel_regularizer=l1reg)(x)\n",
    "\n",
    "    use_feats = kwargs['use_feats']\n",
    "    activation = kwargs.get('use_emb_act', 'linear')\n",
    "\n",
    "    # region id\n",
    "    inp2 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x2 = tf.keras.layers.Embedding(reg_id.max()+1,\n",
    "                                   kwargs.get('e_region', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation(activation)(x2)\n",
    "\n",
    "    # city id\n",
    "    inp3 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x3 = tf.keras.layers.Embedding(city_id.max()+1,\n",
    "                                   kwargs.get('e_city', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Activation(activation)(x3)\n",
    "\n",
    "    # cpeman id\n",
    "    inp4 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x4 = tf.keras.layers.Embedding(cpeman_id.max()+1,\n",
    "                                   kwargs.get('e_cpeman', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp4)\n",
    "    x4 = tf.keras.layers.Flatten()(x4)\n",
    "    x4 = tf.keras.layers.Activation(activation)(x4)\n",
    "\n",
    "    # cpemodname id\n",
    "    inp5 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x5 = tf.keras.layers.Embedding(cpemodname_id.max()+1,\n",
    "                                   kwargs.get('e_cpemodname', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp5)\n",
    "    x5 = tf.keras.layers.Flatten()(x5)\n",
    "    x5 = tf.keras.layers.Activation(activation)(x5)\n",
    "\n",
    "    # cpetype_id id\n",
    "    inp6 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x6 = tf.keras.layers.Embedding(cpetype_id.max()+1,\n",
    "                                   kwargs.get('e_cpetype', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp6)\n",
    "    x6 = tf.keras.layers.Flatten()(x6)\n",
    "    x6 = tf.keras.layers.Activation(activation)(x6)\n",
    "\n",
    "    # price id\n",
    "    inp7 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x7 = tf.keras.layers.Dense(kwargs.get('e_price', emb_size),\n",
    "                               activation='tanh',\n",
    "                               kernel_regularizer=l1reg)(inp7)\n",
    "\n",
    "    # cbagmap\n",
    "    inp8 = tf.keras.layers.Input((cbag_all.shape[-1],), sparse=False)\n",
    "    x8 = tf.keras.layers.Dense(kwargs.get('e_cbag', emb_size*2),\n",
    "                               activation='relu',\n",
    "                               kernel_regularizer=l1reg)(inp8)\n",
    "\n",
    "    # date_id id\n",
    "    inp9 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x9 = tf.keras.layers.Embedding(date_id.max()+1,\n",
    "                                   kwargs.get('e_date', emb_size),\n",
    "                                   embeddings_regularizer=l1reg)(inp9)\n",
    "    x9 = tf.keras.layers.Flatten()(x9)\n",
    "    x9 = tf.keras.layers.Activation(activation)(x9)\n",
    "\n",
    "    # pod_id id\n",
    "    inp10 = tf.keras.layers.Input((1,), sparse=False)\n",
    "    x10 = tf.keras.layers.Embedding(pod_id.max()+1,\n",
    "                                    kwargs.get('e_pod', emb_size),\n",
    "                                    embeddings_regularizer=l1reg)(inp10)\n",
    "    x10 = tf.keras.layers.Flatten()(x10)\n",
    "    x10 = tf.keras.layers.Activation(activation)(x10)\n",
    "\n",
    "    # d2v\n",
    "    inp13 = tf.keras.layers.Input((doc2vec_feats.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_doc', emb_size) == -1:\n",
    "        x13 = inp13\n",
    "    else:\n",
    "        x13 = tf.keras.layers.Dense(kwargs.get('e_doc', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp13)\n",
    "\n",
    "    # clip\n",
    "    inp14 = tf.keras.layers.Input((clipmap_train.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_clip', emb_size) == -1:\n",
    "        x14 = inp14\n",
    "    else:\n",
    "        x14 = tf.keras.layers.Dense(kwargs.get('e_clip', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp14)\n",
    "\n",
    "\n",
    "    # titles\n",
    "    inp15 = tf.keras.layers.Input((titlemap_train.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_title', emb_size*2) == -1:\n",
    "        x15 = inp15\n",
    "    else:\n",
    "        x15 = tf.keras.layers.Dense(kwargs.get('e_title', emb_size*2),\n",
    "                                    activation='relu',\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=l1reg)(inp15)\n",
    "\n",
    "    # html svd\n",
    "    inp16 = tf.keras.layers.Input((htmlmap_train.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_html', emb_size) == -1:\n",
    "        x16 = inp16\n",
    "    else:\n",
    "        x16 = tf.keras.layers.Dense(kwargs.get('e_html', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp16)\n",
    "\n",
    "    # simweb\n",
    "    inp17 = tf.keras.layers.Input((simweb_feats.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_simweb', emb_size) == -1:\n",
    "        x17 = inp17\n",
    "    else:\n",
    "        x17 = tf.keras.layers.Dense(kwargs.get('e_simweb', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp17)\n",
    "\n",
    "    # bigrams\n",
    "    inp18 = tf.keras.layers.Input((bigrams_feats.shape[-1],), sparse=False)\n",
    "    if kwargs.get('e_bigram', emb_size) == -1:\n",
    "        x18 = inp18\n",
    "    else:\n",
    "        x18 = tf.keras.layers.Dense(kwargs.get('e_bigram', emb_size),\n",
    "                                    activation='relu',\n",
    "                                    kernel_regularizer=l1reg)(inp18)\n",
    "\n",
    "    x_extra = [\n",
    "                 x2,\n",
    "                 x3,\n",
    "                 x4,\n",
    "                 x5,\n",
    "                 x6,\n",
    "                 x7,\n",
    "                 x8,\n",
    "                 x9,\n",
    "                 x10,\n",
    "                 x13,\n",
    "                 x14,\n",
    "                 x15,\n",
    "                 x16,\n",
    "                 x17,\n",
    "                 x18,\n",
    "            ]\n",
    "\n",
    "    x_extra = [xx for xx, remain in zip(x_extra, use_feats) if remain]\n",
    "\n",
    "    x_sex0 = tf.keras.layers.concatenate([x] + x_extra)\n",
    "    if kwargs.get('pre_bn', False):\n",
    "        x_sex0 = tf.keras.layers.BatchNormalization(\n",
    "            epsilon=1e-5, momentum=0.1)(x_sex0)\n",
    "    if kwargs.get('pre_dropout', False):\n",
    "        x_sex0 = tf.keras.layers.Dropout(0.1)(x_sex0)\n",
    "    x_age0 = x_sex0\n",
    "\n",
    "\n",
    "    parallel_age = []\n",
    "\n",
    "    nn_act = kwargs.get('nn_act', 'relu')\n",
    "    for _ in range(1):\n",
    "        prev_x_age = [x_age0]\n",
    "        x_age = x_age0\n",
    "        for _ in range(kwargs.get('dense_con_num', 2)):\n",
    "            x2 = tf.keras.layers.Dense(x_age.shape[-1], activation=nn_act,\n",
    "                                       use_bias=True,\n",
    "                                       kernel_regularizer=l1reg)(x_age)\n",
    "            # dense connections\n",
    "            prev_x_age.append(x2)\n",
    "            x_age = tf.keras.layers.add(prev_x_age)\n",
    "            if kwargs.get('bn', False):\n",
    "                x_age = tf.keras.layers.BatchNormalization(\n",
    "                    epsilon=1e-5, momentum=0.1)(x_age)\n",
    "            if kwargs.get('dropout', False)>0:\n",
    "                x_age = tf.keras.layers.Dropout(kwargs.get('dropout'))(x_age)\n",
    "        parallel_age.append(x_age)\n",
    "\n",
    "    if kwargs.get('age_extra_dim', False):\n",
    "        x_age = tf.keras.layers.concatenate([\n",
    "            tf.keras.layers.Dense(kwargs.get('age_extra_dim'),\n",
    "                                  activation=nn_act,\n",
    "                                  use_bias=True,\n",
    "                                  kernel_regularizer=l1reg)(x_age0)\n",
    "        ] + parallel_age)\n",
    "    else:\n",
    "        x_age = parallel_age[0]\n",
    "\n",
    "    if kwargs.get('sex_extra_dim', False):\n",
    "        x_sex = tf.keras.layers.concatenate([\n",
    "            tf.keras.layers.Dense(kwargs.get('sex_extra_dim'),\n",
    "                                  activation=nn_act,\n",
    "                                  use_bias=True,\n",
    "                                  kernel_regularizer=l1reg)(x_sex0)\n",
    "        ] + parallel_age)\n",
    "    else:\n",
    "        x_sex = parallel_age[0]\n",
    "\n",
    "    out1 = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True, name='sex',\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l1(l1_reg))(x_sex)\n",
    "\n",
    "    out2 = tf.keras.layers.Dense(6, activation='softmax', use_bias=True, name='age',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1(l1_reg))(x_age)\n",
    "\n",
    "    inps_extra = [\n",
    "        inp2,\n",
    "        inp3,\n",
    "        inp4,\n",
    "        inp5,\n",
    "        inp6,\n",
    "        inp7,\n",
    "        inp8,\n",
    "        inp9,\n",
    "        inp10,\n",
    "        inp13,\n",
    "        inp14,\n",
    "        inp15,\n",
    "        inp16,\n",
    "        inp17,\n",
    "        inp18\n",
    "    ]\n",
    "    inps_extra = [xx for xx, remain in zip(inps_extra, use_feats) if remain]\n",
    "\n",
    "    model = tf.keras.models.Model([inp] + inps_extra, [out1, out2])\n",
    "\n",
    "    max_weight = max(sex_weight, age_weight)\n",
    "    model.compile(loss={'sex':'binary_crossentropy',\n",
    "                        'age':'categorical_crossentropy'},\n",
    "                  loss_weights={'sex':sex_weight/max_weight,\n",
    "                                'age':age_weight/max_weight},\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=base_lr,\n",
    "                                                     clipvalue=kwargs.get('clipvalue', 2.)),\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_scheduler(base_lr=2e-4, factor=1., offset=0.5):\n",
    "    def scheduler(epoch, lr):\n",
    "        return base_lr*10**(-epoch*factor+offset)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кешируем фолды кроссвалидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c290a4f134a648c5af354348932e6df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = csr_matrix((X_tr.data**0.5, X_tr.nonzero()),\n",
    "                    shape=X_tr.shape,\n",
    "                    dtype=np.float32)\n",
    "\n",
    "y_ohe_age = np.zeros((y_all.size, y_all.max()))\n",
    "y_ohe_age[np.arange(y_all.size), y_all-1] = 1.\n",
    "\n",
    "CACHE = {}\n",
    "for k, (train_ind, test_ind) in enumerate(tqdm(folds)):\n",
    "    # тут выбираем 2-3 случайных номера фолдов\n",
    "    # чтобы подобранные архитектуры были разнообразными\n",
    "    if k not in [3, 4]: continue\n",
    "    train_dat = X_train[train_ind]\n",
    "    train_y_sex = trg_sex[train_ind]\n",
    "    train_y_age = y_ohe_age[train_ind]\n",
    "    val_dat = X_train[test_ind]\n",
    "    val_y_sex = trg_sex[test_ind]\n",
    "    val_y_age = y_ohe_age[test_ind]\n",
    "\n",
    "    train_aux_dat = [train_dat,\n",
    "                     reg_id[all_mask][train_ind, None],\n",
    "                     city_id[all_mask][train_ind, None],\n",
    "                     cpeman_id[all_mask][train_ind, None],\n",
    "                     cpemodname_id[all_mask][train_ind, None],\n",
    "                     cpetype_id[all_mask][train_ind, None],\n",
    "                     price_id[all_mask][train_ind, None]**0.5,\n",
    "                     cbag_all_train[train_ind],\n",
    "                     date_id[all_mask][train_ind, None],\n",
    "                     pod_id[all_mask][train_ind, None],\n",
    "                     doc2vec_feats_train[train_ind],\n",
    "                     clipmap_train[train_ind],\n",
    "                     titlemap_train[train_ind],\n",
    "                     htmlmap_train[train_ind],\n",
    "                     simweb_feats_train[train_ind],\n",
    "                     bigrams_feats_train[train_ind],\n",
    "                    ]\n",
    "\n",
    "    val_aux_dat = [val_dat,\n",
    "                   reg_id[all_mask][test_ind, None],\n",
    "                   city_id[all_mask][test_ind, None],\n",
    "                   cpeman_id[all_mask][test_ind, None],\n",
    "                   cpemodname_id[all_mask][test_ind, None],\n",
    "                   cpetype_id[all_mask][test_ind, None],\n",
    "                   price_id[all_mask][test_ind, None]**0.5,\n",
    "                   cbag_all_train[test_ind],\n",
    "                   date_id[all_mask][test_ind, None],\n",
    "                   pod_id[all_mask][test_ind, None],\n",
    "                   doc2vec_feats_train[test_ind],\n",
    "                   clipmap_train[test_ind],\n",
    "                   titlemap_train[test_ind],\n",
    "                   htmlmap_train[test_ind],\n",
    "                   simweb_feats_train[test_ind],\n",
    "                   bigrams_feats_train[test_ind],\n",
    "                  ]\n",
    "\n",
    "    with gzip.open('oof_scores_260223/101010/%d.pickle.gz'%(k+1), 'rb') as f:\n",
    "        oof_scores = pickle.load(f)\n",
    "\n",
    "    CACHE[k] = [[train_aux_dat, [train_y_sex, train_y_age,\n",
    "                                 oof_scores['sex'], oof_scores['age']]],\n",
    "                [val_aux_dat, [val_y_sex, val_y_age]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мапка признаков для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat2idx_map = dict(e_region=0,\n",
    "                    e_city=1,\n",
    "                    e_cpeman=2,\n",
    "                    e_cpemodname=3,\n",
    "                    e_cpetype=4,\n",
    "                    e_price=5,\n",
    "                    e_date=6,\n",
    "                    e_pod=7,\n",
    "                    e_cbag=8,\n",
    "                    e_doc=9,\n",
    "                    e_clip=10,\n",
    "                    e_title=11,\n",
    "                    e_html=12,\n",
    "                    e_simweb=13,\n",
    "                    e_bigram=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция качества для байесовского поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_state(model, mode='train'):\n",
    "    for i, l in enumerate(model.layers):\n",
    "        model.layers[i].trainable = (mode=='train')\n",
    "    return model\n",
    "\n",
    "def get_score(args, times=2):\n",
    "    BASELINE_SCORE = 47*2\n",
    "    TIMELIMIT = 60*4\n",
    "\n",
    "    # delete useless values\n",
    "    for k, v in args.copy().items():\n",
    "        if v is None:\n",
    "            del args[k]\n",
    "            continue\n",
    "        if k.startswith('e_') or 'extra' in k:\n",
    "            args[k] = int(args[k])\n",
    "\n",
    "    use_feats = [1 for _ in range(len(feat2idx_map))]\n",
    "    for k, v in feat2idx_map.items():\n",
    "        use_feats[v] = int(args.get(k, 100) != 1)\n",
    "\n",
    "    args['use_feats'] = tuple(use_feats)\n",
    "\n",
    "    epochs = args['epochs'] = int(args['epochs']) #4\n",
    "    batch_size = args['batch_size'] = int(args['batch_size']) #128+256\n",
    "    steps_subsample = args['steps_subsample'] = float('%.3f'%args['steps_subsample']) #0.95\n",
    "    base_lr = args['base_lr'] = args['base_lr'] #2e-4\n",
    "    factor = args['factor'] = float('%.3f'%args['factor']) #1.\n",
    "    offset = args['offset'] = float('%.3f'%args['offset']) #0.5\n",
    "    emb_size = args['emb_size'] = int(args['emb_size']) # 64\n",
    "    dense_size = args['dense_size'] = int(args['dense_size']) #1024\n",
    "    sex_weight = args['sex_weight'] = float('%.3f'%args['sex_weight'])\n",
    "    age_weight = args['age_weight'] = float('%.3f'%args['age_weight'])\n",
    "    sex_alpha = args['sex_alpha']\n",
    "    age_alpha = args['age_alpha']\n",
    "    scheduler = get_scheduler(base_lr, factor, offset)\n",
    "\n",
    "    AUX_DAT = [\n",
    "             all_data_sqrt,\n",
    "             reg_id[:, None],\n",
    "             city_id[:, None],\n",
    "             cpeman_id[:, None],\n",
    "             cpemodname_id[:, None],\n",
    "             cpetype_id[:, None],\n",
    "             price_id[:, None]**0.5,\n",
    "             cbag_all,\n",
    "             date_id[:, None],\n",
    "             pod_id[:, None],\n",
    "             doc2vec_feats,\n",
    "             clipmap,\n",
    "             titlemap,\n",
    "             htmlmap,\n",
    "             simweb_feats,\n",
    "             bigrams_feats,\n",
    "    ]\n",
    "    AUX_DAT = [xx for xx, remain in zip(AUX_DAT, [1]+use_feats) if remain]\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    scores = []\n",
    "    seconds = []\n",
    "    f1s = []\n",
    "    rocaucs = []\n",
    "\n",
    "    for k, (train_ind, test_ind) in enumerate(tqdm(folds)):\n",
    "        for _ in range(times):\n",
    "            if k not in CACHE: continue\n",
    "            model_nn = get_model_all(**args)\n",
    "            num_params = sum([w.flatten().size for w in model_nn.get_weights()])\n",
    "            # ранний выход из-за сложности модели\n",
    "            if (batch_size > 200 and args['dense_size'] > 2000) or \\\n",
    "               (num_params > 65_000_000):\n",
    "                return {'loss': 100,\n",
    "                        'loss_variance': 0.5,\n",
    "                        'status': STATUS_OK,\n",
    "                        'params_args': args,\n",
    "                        'spent_time': seconds,\n",
    "                        'scores': [f1s, rocaucs, scores]}\n",
    "\n",
    "            [train_aux_dat, train_y],\\\n",
    "            [val_aux_dat, val_y] = CACHE[k]\n",
    "            train_y_sex, train_y_age, oof_sex, oof_age = train_y\n",
    "            val_y_sex, val_y_age = val_y\n",
    "\n",
    "            train_aux_dat = [xx for xx, remain in zip(train_aux_dat, [1]+use_feats) if remain]\n",
    "            val_aux_dat = [xx for xx, remain in zip(val_aux_dat, [1]+use_feats) if remain]\n",
    "\n",
    "            st_time = time.time()\n",
    "            weights_backup = model_nn.get_weights()\n",
    "\n",
    "            model_nn.fit(train_aux_dat,\n",
    "                         [oof_sex*sex_alpha + train_y_sex*(1 - sex_alpha),\n",
    "                          oof_age*age_alpha + train_y_age*(1 - age_alpha)],\n",
    "                      batch_size=batch_size,\n",
    "                      steps_per_epoch=int(steps_subsample*train_ind.size/batch_size),\n",
    "                      epochs=epochs,\n",
    "                      callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler)],\n",
    "                      verbose=False)\n",
    "\n",
    "            end_time = time.time()\n",
    "            spent_seconds = end_time - st_time\n",
    "            if spent_seconds > TIMELIMIT:\n",
    "                return {'loss': 100,\n",
    "                        'loss_variance': 0.5,\n",
    "                        'status': STATUS_OK,\n",
    "                        'params_args': args,\n",
    "                        'spent_time': seconds,\n",
    "                        'scores': [f1s, rocaucs, scores]}\n",
    "\n",
    "            if args.get('pseudo', False):\n",
    "                # PSEUDO-labelling\n",
    "                model_nn = change_state(model_nn, 'test')\n",
    "                AUX_PREDS = model_nn.predict(AUX_DAT, batch_size=1024)\n",
    "                model_nn = change_state(model_nn, 'train')\n",
    "                model_nn.set_weights(weights_backup)\n",
    "                del weights_backup\n",
    "\n",
    "                tf.keras.backend.set_value(model_nn.optimizer.lr, args.get('pretrain_lr', base_lr))\n",
    "                model_nn.fit(AUX_DAT, AUX_PREDS,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=1,\n",
    "                          verbose=False)\n",
    "\n",
    "                tf.keras.backend.set_value(model_nn.optimizer.lr, base_lr)\n",
    "                model_nn.fit(train_aux_dat,\n",
    "                             [oof_sex*sex_alpha + train_y_sex*(1 - sex_alpha),\n",
    "                              oof_age*age_alpha + train_y_age*(1 - age_alpha)],\n",
    "                          batch_size=batch_size,\n",
    "                          steps_per_epoch=int(steps_subsample*train_ind.size/batch_size),\n",
    "                          epochs=epochs,\n",
    "                          callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler)],\n",
    "                          verbose=False)\n",
    "\n",
    "            model_nn = change_state(model_nn, 'test')\n",
    "            preds = \\\n",
    "            model_nn.predict(val_aux_dat, batch_size=1024)\n",
    "\n",
    "            end_time = time.time()\n",
    "            spent_seconds = end_time - st_time\n",
    "\n",
    "            f1 = \\\n",
    "            f1_score(val_y_age.argmax(axis=1),\n",
    "                     preds[-1].argmax(axis=1), average='weighted')*100\n",
    "            rocauc = \\\n",
    "            roc_auc_score(val_y_sex[val_y_sex!=0.5],\n",
    "                          preds[0].flatten()[val_y_sex!=0.5])*100\n",
    "\n",
    "            score = f1*2 + (rocauc-50)*2*0\n",
    "            scores.append(score - BASELINE_SCORE)\n",
    "            seconds.append(int(spent_seconds))\n",
    "            f1s.append(f1)\n",
    "            rocaucs.append(rocauc)\n",
    "\n",
    "            print(score)\n",
    "            # ранний выход, если не уложились в бюджет времени\n",
    "            if spent_seconds > TIMELIMIT*(1+1.5*int(args.get('pseudo', False))):\n",
    "                return {'loss': 100,\n",
    "                        'loss_variance': 0.5,\n",
    "                        'status': STATUS_OK,\n",
    "                        'params_args': args,\n",
    "                        'spent_time': seconds,\n",
    "                        'scores': [f1s, rocaucs, scores]}\n",
    "            # ранний выход, если конфигурация модели незначительно лучше бейзлайна\n",
    "            if score < (BASELINE_SCORE + 2.):\n",
    "                return {'loss': -np.mean(scores),\n",
    "                        'loss_variance': np.var(scores, ddof=1) if len(scores)>1 else 0.5,\n",
    "                        'status': STATUS_OK,\n",
    "                        'params_args': args,\n",
    "                        'spent_time': seconds,\n",
    "                        'scores': [f1s, rocaucs, scores]}\n",
    "\n",
    "    return {'loss': -np.mean(scores),\n",
    "            'loss_variance': np.var(scores, ddof=1),\n",
    "            'status': STATUS_OK,\n",
    "            'params_args': args,\n",
    "            'spent_time': seconds,\n",
    "            'scores': [f1s, rocaucs, scores]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пространство поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'epochs': hp.quniform('epochs', 2, 4, 1),\n",
    "    'batch_size': hp.quniform('batch_size', 192, 256+128, 64),\n",
    "    'steps_subsample': hp.quniform('steps_subsample', 0.9, 1.0, 0.05),\n",
    "    'base_lr': 10**hp.quniform('base_lr', -5, -2, 0.25),\n",
    "    'clipvalue': hp.quniform('clipvalue', 0.5, 10, 0.5),\n",
    "    'factor': hp.quniform('factor', 1, 2, 0.1),\n",
    "    'offset': hp.quniform('offset', -1., 1., 0.1),\n",
    "    'emb_size': 64*2**hp.quniform('emb_size', -3, 3, 0.25),\n",
    "    'dense_size': 1024*2**hp.quniform('dense_size', -3, 1.5, 0.25),\n",
    "    'sex_weight': hp.quniform('sex_weight', 0.1, 5.0, 0.1),\n",
    "    'age_weight': hp.quniform('age_weight', 0.1, 5.0, 0.1),\n",
    "    'dense_con_num': hp.choice('dense_con_num', [None, 1, 3, 4]),\n",
    "    'pre_bn': hp.choice('pre_bn', [None, True]),\n",
    "    'bn': hp.choice('bn', [None, True]),\n",
    "    'pre_dropout': hp.choice('pre_dropout', [None, True]),\n",
    "    'dropout': hp.choice('dropout', [None, 0.05, 0.1, 0.2]),\n",
    "    'nn_act': hp.choice('nn_act', [None, 'elu', 'tanh', 'sigmoid']),\n",
    "\n",
    "    'pretrain_lr': 10**hp.quniform('pretrain_lr', -5, -2, 0.25),\n",
    "    'pseudo': hp.choice('pseudo', [False]),\n",
    "\n",
    "    'sex_alpha': hp.quniform('sex_alpha', 0.0, 1.0, 0.05),\n",
    "    'age_alpha': hp.quniform('age_alpha', 0.0, 1.0, 0.05),\n",
    "\n",
    "    'l1_reg': 10**hp.quniform('l1_reg', -8, -4, 0.25),\n",
    "\n",
    "    'e_region': hp.choice('b1',\n",
    "                       [None,\n",
    "                        1,\n",
    "                        64*2**hp.quniform('region', -3, 3, 0.25)]),\n",
    "    'e_city': hp.choice('b2',\n",
    "                     [None,\n",
    "                      1,\n",
    "                      64*2**hp.quniform('city', -3, 3, 0.25)]),\n",
    "    'e_cpeman': hp.choice('b3',\n",
    "                       [None,\n",
    "                        1,\n",
    "                        64*2**hp.quniform('cpeman', -3, 3, 0.25)]),\n",
    "    'e_cpemodname': hp.choice('b4',\n",
    "                           [None,\n",
    "                            1,\n",
    "                            64*2**hp.quniform('cpemodname', -3, 3, 0.25)]),\n",
    "    'e_cpetype': hp.choice('b5',\n",
    "                        [None,\n",
    "                         1,\n",
    "                         64*2**hp.quniform('cpetype', -3, 3, 0.25)]),\n",
    "    'e_price': hp.choice('b6',\n",
    "                      [None,\n",
    "                       1,\n",
    "                       64*2**hp.quniform('price', -3, 3, 0.25)]),\n",
    "    'e_date': hp.choice('b7',\n",
    "                     [None,\n",
    "                      1,\n",
    "                      64*2**hp.quniform('date', -3, 3, 0.25)]),\n",
    "    'e_pod': hp.choice('b8',\n",
    "                    [None,\n",
    "                     1,\n",
    "                     64*2**hp.quniform('pod', -3, 3, 0.25)]),\n",
    "    'e_cbag': hp.choice('b10',\n",
    "                     [None,\n",
    "                      1,\n",
    "                      128*2**hp.quniform('cbag', -3, 3, 0.25)]),\n",
    "    'e_title': hp.choice('b9',\n",
    "                      [None,\n",
    "                       1,\n",
    "                       128*2**hp.quniform('title', -3, 3, 0.25)]),\n",
    "    'e_doc': hp.choice('b11',\n",
    "                       [None,\n",
    "                        -1,\n",
    "                        1,\n",
    "                        64*2**hp.quniform('doc', -3, 3, 0.25)]),\n",
    "    'e_clip': hp.choice('b12',\n",
    "                       [None,\n",
    "                        -1,\n",
    "                        1,\n",
    "                        64*2**hp.quniform('clip', -3, 3, 0.25)]),\n",
    "    'e_html': hp.choice('b13',\n",
    "                       [None,\n",
    "                        -1,\n",
    "                        1,\n",
    "                        64*2**hp.quniform('html', -3, 3, 0.25)]),\n",
    "    'e_simweb': hp.choice('b14',\n",
    "                       [None,\n",
    "                        -1,\n",
    "                        1,\n",
    "                        64*2**hp.quniform('simweb', -3, 3, 0.25)]),\n",
    "    'e_bigram': hp.choice('b15',\n",
    "                          [None,\n",
    "                           1,\n",
    "                           128*2**hp.quniform('bigram', -4, 2, 0.25)]),\n",
    "\n",
    "    'age_extra_dim': hp.choice('use_age_extra',\n",
    "                              [None,\n",
    "                               64*2**hp.quniform('age_extra_dim', -5, 5, 0.5)]),\n",
    "    'sex_extra_dim': hp.choice('use_sex_extra',\n",
    "                              [None,\n",
    "                               64*2**hp.quniform('sex_extra_dim', -5, 5, 0.5)]),\n",
    "\n",
    "    'use_emb_act': hp.choice('use_emb_act', [None, 'relu', 'elu', 'tanh'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Байесовский поиск с чекпоинтами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "{'age_alpha': 0.55, 'age_extra_dim': 45, 'age_weight': 4.4, 'base_lr': 0.0031622776601683794, 'batch_size': 192, 'bn': True, 'clipvalue': 1.0, 'dense_size': 861, 'dropout': 0.05, 'e_bigram': 128, 'e_cbag': 256, 'e_city': 128, 'e_cpeman': 19, 'e_cpemodname': 1, 'e_cpetype': 64, 'e_doc': -1, 'e_html': -1, 'e_region': 26, 'emb_size': 90, 'epochs': 4, 'factor': 1.2, 'l1_reg': 3.162277660168379e-07, 'nn_act': 'sigmoid', 'offset': -0.3, 'pre_dropout': True, 'pretrain_lr': 0.0031622776601683794, 'pseudo': False, 'sex_alpha': 0.65, 'sex_weight': 1.3, 'steps_subsample': 1.0, 'use_emb_act': 'tanh', 'use_feats': (1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)}\n",
      "100%|█████████▉| 277/278 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1b5d245c0d4fe7adac8b0a579c49dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_726/dense_8730/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_726/dense_8730/embedding_lookup_sparse/Reshape:0\", shape=(None, 861), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_726/dense_8730/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_726/dense_8734/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_726/dense_8734/embedding_lookup_sparse/Reshape:0\", shape=(None, 180), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_726/dense_8734/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_726/dense_8732/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_726/dense_8732/embedding_lookup_sparse/Reshape:0\", shape=(None, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_726/dense_8732/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.77138968597038                                        \n",
      "100%|█████████▉| 277/278 [03:29<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_727/dense_8740/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_727/dense_8740/embedding_lookup_sparse/Reshape:0\", shape=(None, 861), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_727/dense_8740/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_727/dense_8744/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_727/dense_8744/embedding_lookup_sparse/Reshape:0\", shape=(None, 180), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_727/dense_8744/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_727/dense_8742/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_727/dense_8742/embedding_lookup_sparse/Reshape:0\", shape=(None, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_727/dense_8742/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.43578456472916                                        \n",
      "100%|█████████▉| 277/278 [07:07<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_728/dense_8750/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_728/dense_8750/embedding_lookup_sparse/Reshape:0\", shape=(None, 861), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_728/dense_8750/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_728/dense_8754/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_728/dense_8754/embedding_lookup_sparse/Reshape:0\", shape=(None, 180), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_728/dense_8754/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_728/dense_8752/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_728/dense_8752/embedding_lookup_sparse/Reshape:0\", shape=(None, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_728/dense_8752/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.00203960897477                                        \n",
      "100%|█████████▉| 277/278 [10:48<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_729/dense_8760/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_729/dense_8760/embedding_lookup_sparse/Reshape:0\", shape=(None, 861), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_729/dense_8760/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_729/dense_8764/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_729/dense_8764/embedding_lookup_sparse/Reshape:0\", shape=(None, 180), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_729/dense_8764/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n",
      "/data/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_729/dense_8762/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_729/dense_8762/embedding_lookup_sparse/Reshape:0\", shape=(None, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_729/dense_8762/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 277/278 [13:44<?, ?trial/s, best loss=?]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "trials_file = 'trials280323age_%s'%(''.join(map(str, sorted(list(CACHE.keys())))))+\\\n",
    "              '_%d.pickle.gz'\n",
    "\n",
    "n_checkpoints = 2\n",
    "\n",
    "trials_length = 0\n",
    "for i in range(n_checkpoints):\n",
    "    if os.path.exists(trials_file%(i+1)):\n",
    "        with gzip.open(trials_file%(i+1), 'rb') as f:\n",
    "            tmp = pickle.load(f)\n",
    "            if len(tmp.trials) > trials_length:\n",
    "                trials_length = len(tmp.trials)\n",
    "                trials = tmp\n",
    "        print('Loaded %s'%(trials_file%(i+1)))\n",
    "\n",
    "\n",
    "suggester = partial(tpe.suggest, n_startup_jobs=50, n_EI_candidates=25, gamma=0.25)\n",
    "for k_iter in range(1000):\n",
    "    print(len(trials.trials))\n",
    "    try:\n",
    "        best = fmin(get_score, space, suggester,\n",
    "                    max_evals=len(trials.trials) + 1,\n",
    "                    rstate=None, trials=trials)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    with gzip.open(trials_file%(k_iter%n_checkpoints+1), 'wb') as f:\n",
    "        pickle.dump(trials, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if k_iter % 3 == 0:\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод 3 лучших конфигураций моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 262,\n",
       "  'spec': None,\n",
       "  'result': {'loss': -3.1522602500615413,\n",
       "   'loss_variance': 0.04850040593790047,\n",
       "   'status': 'ok',\n",
       "   'params_args': {'age_alpha': 0.7000000000000001,\n",
       "    'age_extra_dim': 32,\n",
       "    'age_weight': 4.2,\n",
       "    'base_lr': 0.0017782794100389228,\n",
       "    'batch_size': 256,\n",
       "    'bn': True,\n",
       "    'clipvalue': 1.0,\n",
       "    'dense_size': 608,\n",
       "    'dropout': 0.05,\n",
       "    'e_bigram': 107,\n",
       "    'e_cbag': 181,\n",
       "    'e_city': 90,\n",
       "    'e_cpeman': 38,\n",
       "    'e_cpemodname': 1,\n",
       "    'e_cpetype': 90,\n",
       "    'e_doc': -1,\n",
       "    'e_html': -1,\n",
       "    'e_pod': 64,\n",
       "    'e_region': 90,\n",
       "    'emb_size': 76,\n",
       "    'epochs': 4,\n",
       "    'factor': 1.0,\n",
       "    'l1_reg': 3.162277660168379e-07,\n",
       "    'nn_act': 'elu',\n",
       "    'offset': 0.2,\n",
       "    'pre_dropout': True,\n",
       "    'pretrain_lr': 0.0001,\n",
       "    'pseudo': False,\n",
       "    'sex_alpha': 0.55,\n",
       "    'sex_weight': 2.2,\n",
       "    'steps_subsample': 1.0,\n",
       "    'use_emb_act': 'tanh',\n",
       "    'use_feats': (1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)},\n",
       "   'spent_time': [167, 166, 165, 176],\n",
       "   'scores': [[48.67788268038374,\n",
       "     48.48022272699535,\n",
       "     48.481538549424805,\n",
       "     48.66487654331919],\n",
       "    [88.73756628213434,\n",
       "     88.8010848588893,\n",
       "     88.93002690149332,\n",
       "     88.87464826412041],\n",
       "    [3.355765360767478,\n",
       "     2.960445453990701,\n",
       "     2.9630770988496096,\n",
       "     3.3297530866383767]]},\n",
       "  'misc': {'tid': 262,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'age_alpha': [262],\n",
       "    'age_extra_dim': [262],\n",
       "    'age_weight': [262],\n",
       "    'b1': [262],\n",
       "    'b10': [262],\n",
       "    'b11': [262],\n",
       "    'b12': [262],\n",
       "    'b13': [262],\n",
       "    'b14': [262],\n",
       "    'b15': [262],\n",
       "    'b2': [262],\n",
       "    'b3': [262],\n",
       "    'b4': [262],\n",
       "    'b5': [262],\n",
       "    'b6': [262],\n",
       "    'b7': [262],\n",
       "    'b8': [262],\n",
       "    'b9': [262],\n",
       "    'base_lr': [262],\n",
       "    'batch_size': [262],\n",
       "    'bigram': [262],\n",
       "    'bn': [262],\n",
       "    'cbag': [262],\n",
       "    'city': [262],\n",
       "    'clip': [],\n",
       "    'clipvalue': [262],\n",
       "    'cpeman': [262],\n",
       "    'cpemodname': [],\n",
       "    'cpetype': [262],\n",
       "    'date': [],\n",
       "    'dense_con_num': [262],\n",
       "    'dense_size': [262],\n",
       "    'doc': [],\n",
       "    'dropout': [262],\n",
       "    'emb_size': [262],\n",
       "    'epochs': [262],\n",
       "    'factor': [262],\n",
       "    'html': [],\n",
       "    'l1_reg': [262],\n",
       "    'nn_act': [262],\n",
       "    'offset': [262],\n",
       "    'pod': [262],\n",
       "    'pre_bn': [262],\n",
       "    'pre_dropout': [262],\n",
       "    'pretrain_lr': [262],\n",
       "    'price': [],\n",
       "    'pseudo': [262],\n",
       "    'region': [262],\n",
       "    'sex_alpha': [262],\n",
       "    'sex_extra_dim': [],\n",
       "    'sex_weight': [262],\n",
       "    'simweb': [],\n",
       "    'steps_subsample': [262],\n",
       "    'title': [],\n",
       "    'use_age_extra': [262],\n",
       "    'use_emb_act': [262],\n",
       "    'use_sex_extra': [262]},\n",
       "   'vals': {'age_alpha': [0.7000000000000001],\n",
       "    'age_extra_dim': [-1.0],\n",
       "    'age_weight': [4.2],\n",
       "    'b1': [2],\n",
       "    'b10': [2],\n",
       "    'b11': [1],\n",
       "    'b12': [0],\n",
       "    'b13': [1],\n",
       "    'b14': [0],\n",
       "    'b15': [2],\n",
       "    'b2': [2],\n",
       "    'b3': [2],\n",
       "    'b4': [1],\n",
       "    'b5': [2],\n",
       "    'b6': [0],\n",
       "    'b7': [0],\n",
       "    'b8': [2],\n",
       "    'b9': [0],\n",
       "    'base_lr': [-2.75],\n",
       "    'batch_size': [256.0],\n",
       "    'bigram': [-0.25],\n",
       "    'bn': [1],\n",
       "    'cbag': [0.5],\n",
       "    'city': [0.5],\n",
       "    'clip': [],\n",
       "    'clipvalue': [1.0],\n",
       "    'cpeman': [-0.75],\n",
       "    'cpemodname': [],\n",
       "    'cpetype': [0.5],\n",
       "    'date': [],\n",
       "    'dense_con_num': [0],\n",
       "    'dense_size': [-0.75],\n",
       "    'doc': [],\n",
       "    'dropout': [1],\n",
       "    'emb_size': [0.25],\n",
       "    'epochs': [4.0],\n",
       "    'factor': [1.0],\n",
       "    'html': [],\n",
       "    'l1_reg': [-6.5],\n",
       "    'nn_act': [1],\n",
       "    'offset': [0.2],\n",
       "    'pod': [-0.0],\n",
       "    'pre_bn': [0],\n",
       "    'pre_dropout': [1],\n",
       "    'pretrain_lr': [-4.0],\n",
       "    'price': [],\n",
       "    'pseudo': [0],\n",
       "    'region': [0.5],\n",
       "    'sex_alpha': [0.55],\n",
       "    'sex_extra_dim': [],\n",
       "    'sex_weight': [2.2],\n",
       "    'simweb': [],\n",
       "    'steps_subsample': [1.0],\n",
       "    'title': [],\n",
       "    'use_age_extra': [1],\n",
       "    'use_emb_act': [3],\n",
       "    'use_sex_extra': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2023, 3, 29, 7, 31, 56, 474000),\n",
       "  'refresh_time': datetime.datetime(2023, 3, 29, 7, 43, 14, 717000)},\n",
       " {'state': 2,\n",
       "  'tid': 270,\n",
       "  'spec': None,\n",
       "  'result': {'loss': -2.8521820653440706,\n",
       "   'loss_variance': 0.16249974528051161,\n",
       "   'status': 'ok',\n",
       "   'params_args': {'age_alpha': 0.7000000000000001,\n",
       "    'age_extra_dim': 32,\n",
       "    'age_weight': 3.9,\n",
       "    'base_lr': 0.0031622776601683794,\n",
       "    'batch_size': 256,\n",
       "    'bn': True,\n",
       "    'clipvalue': 1.0,\n",
       "    'dense_size': 861,\n",
       "    'dropout': 0.05,\n",
       "    'e_bigram': 128,\n",
       "    'e_cbag': 304,\n",
       "    'e_city': 152,\n",
       "    'e_cpeman': 19,\n",
       "    'e_cpemodname': 1,\n",
       "    'e_cpetype': 32,\n",
       "    'e_doc': -1,\n",
       "    'e_html': -1,\n",
       "    'e_pod': 53,\n",
       "    'e_region': 53,\n",
       "    'emb_size': 64,\n",
       "    'epochs': 4,\n",
       "    'factor': 1.1,\n",
       "    'l1_reg': 1e-07,\n",
       "    'nn_act': 'elu',\n",
       "    'offset': 0.0,\n",
       "    'pre_dropout': True,\n",
       "    'pretrain_lr': 0.00031622776601683794,\n",
       "    'pseudo': False,\n",
       "    'sex_alpha': 0.75,\n",
       "    'sex_weight': 2.2,\n",
       "    'steps_subsample': 1.0,\n",
       "    'use_emb_act': 'tanh',\n",
       "    'use_feats': (1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)},\n",
       "   'spent_time': [188, 181, 176, 196],\n",
       "   'scores': [[48.464731139935715,\n",
       "     48.20964581354651,\n",
       "     48.34510260990392,\n",
       "     48.684884567302],\n",
       "    [88.49410943218328,\n",
       "     88.53822181437621,\n",
       "     88.80000922304181,\n",
       "     88.90061255891864],\n",
       "    [2.92946227987143,\n",
       "     2.419291627093017,\n",
       "     2.6902052198078366,\n",
       "     3.369769134603999]]},\n",
       "  'misc': {'tid': 270,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'age_alpha': [270],\n",
       "    'age_extra_dim': [270],\n",
       "    'age_weight': [270],\n",
       "    'b1': [270],\n",
       "    'b10': [270],\n",
       "    'b11': [270],\n",
       "    'b12': [270],\n",
       "    'b13': [270],\n",
       "    'b14': [270],\n",
       "    'b15': [270],\n",
       "    'b2': [270],\n",
       "    'b3': [270],\n",
       "    'b4': [270],\n",
       "    'b5': [270],\n",
       "    'b6': [270],\n",
       "    'b7': [270],\n",
       "    'b8': [270],\n",
       "    'b9': [270],\n",
       "    'base_lr': [270],\n",
       "    'batch_size': [270],\n",
       "    'bigram': [270],\n",
       "    'bn': [270],\n",
       "    'cbag': [270],\n",
       "    'city': [270],\n",
       "    'clip': [],\n",
       "    'clipvalue': [270],\n",
       "    'cpeman': [270],\n",
       "    'cpemodname': [],\n",
       "    'cpetype': [270],\n",
       "    'date': [],\n",
       "    'dense_con_num': [270],\n",
       "    'dense_size': [270],\n",
       "    'doc': [],\n",
       "    'dropout': [270],\n",
       "    'emb_size': [270],\n",
       "    'epochs': [270],\n",
       "    'factor': [270],\n",
       "    'html': [],\n",
       "    'l1_reg': [270],\n",
       "    'nn_act': [270],\n",
       "    'offset': [270],\n",
       "    'pod': [270],\n",
       "    'pre_bn': [270],\n",
       "    'pre_dropout': [270],\n",
       "    'pretrain_lr': [270],\n",
       "    'price': [],\n",
       "    'pseudo': [270],\n",
       "    'region': [270],\n",
       "    'sex_alpha': [270],\n",
       "    'sex_extra_dim': [],\n",
       "    'sex_weight': [270],\n",
       "    'simweb': [],\n",
       "    'steps_subsample': [270],\n",
       "    'title': [],\n",
       "    'use_age_extra': [270],\n",
       "    'use_emb_act': [270],\n",
       "    'use_sex_extra': [270]},\n",
       "   'vals': {'age_alpha': [0.7000000000000001],\n",
       "    'age_extra_dim': [-1.0],\n",
       "    'age_weight': [3.9000000000000004],\n",
       "    'b1': [2],\n",
       "    'b10': [2],\n",
       "    'b11': [1],\n",
       "    'b12': [0],\n",
       "    'b13': [1],\n",
       "    'b14': [0],\n",
       "    'b15': [2],\n",
       "    'b2': [2],\n",
       "    'b3': [2],\n",
       "    'b4': [1],\n",
       "    'b5': [2],\n",
       "    'b6': [0],\n",
       "    'b7': [0],\n",
       "    'b8': [2],\n",
       "    'b9': [0],\n",
       "    'base_lr': [-2.5],\n",
       "    'batch_size': [256.0],\n",
       "    'bigram': [-0.0],\n",
       "    'bn': [1],\n",
       "    'cbag': [1.25],\n",
       "    'city': [1.25],\n",
       "    'clip': [],\n",
       "    'clipvalue': [1.0],\n",
       "    'cpeman': [-1.75],\n",
       "    'cpemodname': [],\n",
       "    'cpetype': [-1.0],\n",
       "    'date': [],\n",
       "    'dense_con_num': [0],\n",
       "    'dense_size': [-0.25],\n",
       "    'doc': [],\n",
       "    'dropout': [1],\n",
       "    'emb_size': [0.0],\n",
       "    'epochs': [4.0],\n",
       "    'factor': [1.1],\n",
       "    'html': [],\n",
       "    'l1_reg': [-7.0],\n",
       "    'nn_act': [1],\n",
       "    'offset': [0.0],\n",
       "    'pod': [-0.25],\n",
       "    'pre_bn': [0],\n",
       "    'pre_dropout': [1],\n",
       "    'pretrain_lr': [-3.5],\n",
       "    'price': [],\n",
       "    'pseudo': [0],\n",
       "    'region': [-0.25],\n",
       "    'sex_alpha': [0.75],\n",
       "    'sex_extra_dim': [],\n",
       "    'sex_weight': [2.2],\n",
       "    'simweb': [],\n",
       "    'steps_subsample': [1.0],\n",
       "    'title': [],\n",
       "    'use_age_extra': [1],\n",
       "    'use_emb_act': [3],\n",
       "    'use_sex_extra': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2023, 3, 29, 8, 18, 34, 639000),\n",
       "  'refresh_time': datetime.datetime(2023, 3, 29, 8, 30, 58, 747000)},\n",
       " {'state': 2,\n",
       "  'tid': 264,\n",
       "  'spec': None,\n",
       "  'result': {'loss': -2.678504514286203,\n",
       "   'loss_variance': 0.2193158993052273,\n",
       "   'status': 'ok',\n",
       "   'params_args': {'age_alpha': 0.7000000000000001,\n",
       "    'age_extra_dim': 45,\n",
       "    'age_weight': 4.0,\n",
       "    'base_lr': 0.0017782794100389228,\n",
       "    'batch_size': 256,\n",
       "    'bn': True,\n",
       "    'clipvalue': 1.0,\n",
       "    'dense_size': 861,\n",
       "    'dropout': 0.05,\n",
       "    'e_bigram': 107,\n",
       "    'e_cbag': 215,\n",
       "    'e_city': 152,\n",
       "    'e_cpeman': 16,\n",
       "    'e_cpemodname': 1,\n",
       "    'e_cpetype': 90,\n",
       "    'e_doc': -1,\n",
       "    'e_html': -1,\n",
       "    'e_pod': 53,\n",
       "    'e_region': 107,\n",
       "    'emb_size': 64,\n",
       "    'epochs': 4,\n",
       "    'factor': 1.1,\n",
       "    'l1_reg': 1e-07,\n",
       "    'nn_act': 'elu',\n",
       "    'offset': 0.2,\n",
       "    'pre_dropout': True,\n",
       "    'pretrain_lr': 0.00017782794100389227,\n",
       "    'pseudo': False,\n",
       "    'sex_alpha': 0.55,\n",
       "    'sex_weight': 2.2,\n",
       "    'steps_subsample': 1.0,\n",
       "    'use_emb_act': 'tanh',\n",
       "    'use_feats': (1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)},\n",
       "   'spent_time': [186, 181, 180, 183],\n",
       "   'scores': [[48.68489168962248,\n",
       "     48.265912732169234,\n",
       "     48.238935259290635,\n",
       "     48.167269347490056],\n",
       "    [88.64053731418518,\n",
       "     88.71278562824037,\n",
       "     88.69099487405107,\n",
       "     88.86688515537102],\n",
       "    [3.3697833792449643,\n",
       "     2.5318254643384677,\n",
       "     2.4778705185812697,\n",
       "     2.334538694980111]]},\n",
       "  'misc': {'tid': 264,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'age_alpha': [264],\n",
       "    'age_extra_dim': [264],\n",
       "    'age_weight': [264],\n",
       "    'b1': [264],\n",
       "    'b10': [264],\n",
       "    'b11': [264],\n",
       "    'b12': [264],\n",
       "    'b13': [264],\n",
       "    'b14': [264],\n",
       "    'b15': [264],\n",
       "    'b2': [264],\n",
       "    'b3': [264],\n",
       "    'b4': [264],\n",
       "    'b5': [264],\n",
       "    'b6': [264],\n",
       "    'b7': [264],\n",
       "    'b8': [264],\n",
       "    'b9': [264],\n",
       "    'base_lr': [264],\n",
       "    'batch_size': [264],\n",
       "    'bigram': [264],\n",
       "    'bn': [264],\n",
       "    'cbag': [264],\n",
       "    'city': [264],\n",
       "    'clip': [],\n",
       "    'clipvalue': [264],\n",
       "    'cpeman': [264],\n",
       "    'cpemodname': [],\n",
       "    'cpetype': [264],\n",
       "    'date': [],\n",
       "    'dense_con_num': [264],\n",
       "    'dense_size': [264],\n",
       "    'doc': [],\n",
       "    'dropout': [264],\n",
       "    'emb_size': [264],\n",
       "    'epochs': [264],\n",
       "    'factor': [264],\n",
       "    'html': [],\n",
       "    'l1_reg': [264],\n",
       "    'nn_act': [264],\n",
       "    'offset': [264],\n",
       "    'pod': [264],\n",
       "    'pre_bn': [264],\n",
       "    'pre_dropout': [264],\n",
       "    'pretrain_lr': [264],\n",
       "    'price': [],\n",
       "    'pseudo': [264],\n",
       "    'region': [264],\n",
       "    'sex_alpha': [264],\n",
       "    'sex_extra_dim': [],\n",
       "    'sex_weight': [264],\n",
       "    'simweb': [],\n",
       "    'steps_subsample': [264],\n",
       "    'title': [],\n",
       "    'use_age_extra': [264],\n",
       "    'use_emb_act': [264],\n",
       "    'use_sex_extra': [264]},\n",
       "   'vals': {'age_alpha': [0.7000000000000001],\n",
       "    'age_extra_dim': [-0.5],\n",
       "    'age_weight': [4.0],\n",
       "    'b1': [2],\n",
       "    'b10': [2],\n",
       "    'b11': [1],\n",
       "    'b12': [0],\n",
       "    'b13': [1],\n",
       "    'b14': [0],\n",
       "    'b15': [2],\n",
       "    'b2': [2],\n",
       "    'b3': [2],\n",
       "    'b4': [1],\n",
       "    'b5': [2],\n",
       "    'b6': [0],\n",
       "    'b7': [0],\n",
       "    'b8': [2],\n",
       "    'b9': [0],\n",
       "    'base_lr': [-2.75],\n",
       "    'batch_size': [256.0],\n",
       "    'bigram': [-0.25],\n",
       "    'bn': [1],\n",
       "    'cbag': [0.75],\n",
       "    'city': [1.25],\n",
       "    'clip': [],\n",
       "    'clipvalue': [1.0],\n",
       "    'cpeman': [-2.0],\n",
       "    'cpemodname': [],\n",
       "    'cpetype': [0.5],\n",
       "    'date': [],\n",
       "    'dense_con_num': [0],\n",
       "    'dense_size': [-0.25],\n",
       "    'doc': [],\n",
       "    'dropout': [1],\n",
       "    'emb_size': [0.0],\n",
       "    'epochs': [4.0],\n",
       "    'factor': [1.1],\n",
       "    'html': [],\n",
       "    'l1_reg': [-7.0],\n",
       "    'nn_act': [1],\n",
       "    'offset': [0.2],\n",
       "    'pod': [-0.25],\n",
       "    'pre_bn': [0],\n",
       "    'pre_dropout': [1],\n",
       "    'pretrain_lr': [-3.75],\n",
       "    'price': [],\n",
       "    'pseudo': [0],\n",
       "    'region': [0.75],\n",
       "    'sex_alpha': [0.55],\n",
       "    'sex_extra_dim': [],\n",
       "    'sex_weight': [2.2],\n",
       "    'simweb': [],\n",
       "    'steps_subsample': [1.0],\n",
       "    'title': [],\n",
       "    'use_age_extra': [1],\n",
       "    'use_emb_act': [3],\n",
       "    'use_sex_extra': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2023, 3, 29, 7, 46, 53, 674000),\n",
       "  'refresh_time': datetime.datetime(2023, 3, 29, 7, 59, 7, 548000)}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold%3 <=> 3 4\n",
    "sorted(trials.trials, key=lambda x: x.get('result', {'loss':100}).get('loss', 100))[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично подбираются гиперпараметры для максимизации пола и для разных сочетаний поднаборов фолдов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
